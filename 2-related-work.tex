\section{Related work}
\label{relatedwork}

Web crawlers are complex software systems which often combine techniques 
from various disciplines in computer science. Our work on the BlogForever 
crawler is related to the fields of web data extraction, distributed 
computing and natural language processing. In the literature on web 
data extraction, the word \emph{wrapper} is commonly used to designate 
procedures to extract structured data from unstructured documents. 
We did not use this word in the present paper in favour of the term 
\emph{extraction rule}, which better reflects our implementation and 
is decoupled from software that concretely performs the extraction.

A common approach in web data extraction is to manually build wrappers 
for the targeted websites. This approach has been proposed for the 
crawler discussed in \cite{faheem2012intelligent} which automatically 
assigns web sites to predefined categories and gets the appropriate 
wrapper from a static knowledge base. The limiting factor in this type 
of approach is the substantial amount of manual work needed to write and 
maintain the wrappers, which is not compatible with the increasing size 
and diversity of the web. Several projects try to simplify this process 
and provide various degrees of automation. This is the case of the 
Stalker algorithm \cite{stalker} which generates wrappers based on 
user-labelled training examples. Some commercial solutions such as 
the Lixto project \cite{lixto} simplify the task of building wrappers 
by offering a complete integrated development environment where the 
training data set is obtained via a graphical user interface.

As an alternative to dedicated software for the creation and maintenance 
of wrappers, some query languages have been designed specifically for 
wrappers. These languages rely on their users to manually identify 
the structure of the data to be extracted. This structure can then 
be formalised as a small declarative program, which can then be turned
into an concrete wrapper by an execution engine. The OXPath language 
\cite{oxpath2013} is an interesting extension to XPath designed to 
incorporate interaction in the extraction process. It supports simulated
user actions such as filling forms or clicking buttons to obtain 
information that would not be accessible otherwise. Another extension of 
XPath, called Spatial XPath \cite{sxpath2010}, allows to write spacial 
rules in the extraction queries. The execution engine embeds a complete 
web browser which computes the visual representation of the page.

Fully automated solutions use different techniques to identify and 
extract information directly from the structure and content of the web 
page, without the need of any manual intervention. The Boilerpipe project 
\cite{kohlschuetter2010} (which is also used in our evaluation) uses text 
density analysis to extract the main article of a web page. The approach 
presented in \cite{treeedit} is based on a tree structure analysis of 
pages with similar templates, such as news web sites or blogs. Automatic 
solutions have also been designed specifically for blogs. Similarly to 
our approach, Oita and Senellart \cite{oita2010} describe a procedure 
to automatically build wrappers by matching web feed articles to
HTML pages. This work was further extended by Gkotsis et al. with 
a focus on extracting content anterior to the one indexed in web 
feeds~\cite{gkotsis2013}. They also report to have successfully extracted 
blog post titles, publication dates and authors, but their approach 
is less generic than the one for the extraction of articles. Finally, 
neither \cite{oita2010} nor \cite{gkotsis2013} provide complexity 
analysis which we believe to be essential before using an algorithm 
in production.

One interesting research direction is the one of large scale distributed 
crawlers. Mercator \cite{heydon99mercator}, UbiCrawler \cite{boldi2003} 
and the crawler discussed in \cite{shkapenyuk2002} are examples of 
successful distributed crawlers. The associated articles provide useful 
information regarding the challenges encountered when working on a 
distributed architecture. One of the core issues when scaling out seems 
to be in sharing the list of URLs that have already been visited and 
those that need to be visited next. While \cite{heydon99mercator} and 
\cite{shkapenyuk2002} rely on a central node to hold this information, 
\cite{boldi2003} uses a fully distributed architecture where URLs are 
divided among nodes using consistent hashing. Both of these approaches 
require the crawlers to implement complex mechanisms to achieve 
fault tolerance. The BlogForever Crawler does not have to address this
issue as it is already Handled by the BlogForever back-end system which
is responsible for task and state management. In addition,
since we process web pages on the fly and directly emit the extracted 
content to the back-end, there is no need for persistent storage on 
the crawler's side. This removes one layer of complexity when compared 
to general crawlers which need to use a distributed file system 
(\cite{shkapenyuk2002} uses NFS, \cite{berger2011} uses HDFS) or 
implement an aggregation mechanism to further exploit the collected data. 
Our design is similar to the distributed active object pattern presented 
in \cite{activeobject1996}, which is further simplified by the fact that 
the state of the crawler instances is not kept between crawls.
