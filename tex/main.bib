
@misc{disqus2014,
	url = {http://disqus.com/websites/},
	author = {Disqus},
	year = {2014},
	howpublished = {http://disqus.com/websites/}
},

@techreport{najork2001,
	title = {High-performance web crawling},
	abstract = {{SRC{\textquoteright}s} charter is to advance the state of the art in computer systems by doing basic and applied research in support of our company{\textquoteright}s business objectives. Our interests and projects span scalable systems (including hardware, networking, distributed systems, and programming-language technology), the Internet (including the Web, e-commerce, and information retrieval), and human/computer interaction (including user-interface technology, computer-based appliances, and mobile computing). {SRC} was established in 1984 by Digital Equipment Corporation. We test the value of our ideas by building hardware and software prototypes and assessing their utility in realistic settings. Interesting systems are too complex to be evaluated solely in the abstract; practical use enables us to investigate their properties in depth. This experience is useful in the short term in refining our designs and invaluable in the long term in advancing our knowledge. Most of the major advances in information systems have come through this approach, including personal computing, distributed systems, and the Internet. We also perform complementary work of a more mathematical character. Some of},
	institution = {{SRC} Research Report 173, Compaq Systems Research},
	author = {Najork, Marc and Heydon, Allan and Najork, Marc and Heydon, Allan},
	year = {2001}
},

@inproceedings{muslea1999,
	address = {New York, {NY}, {USA}},
	series = {{AGENTS} '99},
	title = {A hierarchical approach to wrapper induction},
	isbn = {{1-58113-066-X}},
	url = {http://doi.acm.org/10.1145/301136.301191},
	doi = {10.1145/301136.301191},
	booktitle = {Proceedings of the third annual conference on Autonomous Agents},
	publisher = {{ACM}},
	author = {Muslea, Ion and Minton, Steve and Knoblock, Craig},
	year = {1999},
	pages = {190{\textendash}197}
},

@misc{anderson2012,
	type = {webpage},
	title = {A Tale of a Disappearing Website {\textbar} The Signal: Digital Preservation},
	shorttitle = {A Tale of a Disappearing Website {\textbar} The Signal},
	url = {http://blogs.loc.gov/digitalpreservation/2012/01/a-tale-of-a-disappearing-website/},
	abstract = {A Tale of a Disappearing Website. A blog post at {"The} Signal: Digital Preservation" on 2012-01-17.},
	author = {Anderson, Martha},
	year = {2012},
	keywords = {Blogs, Digital Content, National Digital Information Infrastructure and Preservation Program, {NDIIPP}, Personal Archiving},
	howpublished = {http://blogs.loc.gov/digitalpreservation/2012/01/a-tale-of-a-disappearing-website/}
},

@article{furche2013,
	title = {{OXPath:} A language for scalable data extraction, automation, and crawling on the deep web},
	volume = {22},
	issn = {1066-8888},
	shorttitle = {{OXPath}},
	url = {http://dx.doi.org/10.1007/s00778-012-0286-6},
	doi = {10.1007/s00778-012-0286-6},
	abstract = {The evolution of the web has outpaced itself: A growing wealth of information and increasingly sophisticated interfaces necessitate automated processing, yet existing automation and data extraction technologies have been overwhelmed by this very growth. To address this trend, we identify four key requirements for web data extraction, automation, and (focused) web crawling: (1) interact with sophisticated web application interfaces, (2) precisely capture the relevant data to be extracted, (3) scale with the number of visited pages, and (4) readily embed into existing web technologies. We introduce {OXPath} as an extension of {XPath} for interacting with web applications and extracting data thus revealed--matching all the above requirements. {OXPath's} page-at-a-time evaluation guarantees memory use independent of the number of visited pages, yet remains polynomial in time. We experimentally validate the theoretical complexity and demonstrate that {OXPath's} resource consumption is dominated by page rendering in the underlying browser. With an extensive study of sublanguages and properties of {OXPath}, we pinpoint the effect of specific features on evaluation performance. Our experiments show that {OXPath} outperforms existing commercial and academic data extraction tools by a wide margin.},
	number = {1},
	journal = {The {VLDB} Journal},
	author = {Furche, Tim and Gottlob, Georg and Grasso, Giovanni and Schallhart, Christian and Sellers, Andrew},
	month = feb,
	year = {2013},
	keywords = {{AJAX}, Automation, Crawling, Data extraction, {DOM}, Web applications, Web extraction, {XPath}},
	pages = {47{\textendash}72}
},

@article{dudani1976,
	title = {The {Distance-Weighted} {k-Nearest-Neighbor} Rule},
	volume = {{SMC-6}},
	issn = {0018-9472},
	doi = {10.1109/TSMC.1976.5408784},
	abstract = {Among the simplest and most intuitively appealing classes of nonprobabilistic classification procedures are those that weight the evidence of nearby sample observations most heavily. More specifically, one might wish to weight the evidence of a neighbor close to an unclassified observation more heavily than the evidence of another neighbor which is at a greater distance from the unclassified observation. One such classification rule is described which makes use of a neighbor weighting function for the purpose of assigning a class to an unclassified sample. The admissibility of such a rule is also considered.},
	number = {4},
	journal = {{IEEE} Transactions on Systems, Man and Cybernetics},
	author = {Dudani, Sahibsingh A.},
	year = {1976},
	pages = {325--327}
},

@misc{goose2012,
	url = {https://github.com/GravityLabs/goose},
	abstract = {goose - Html Content / Article Extractor in Scala - open sourced from Gravity Labs - http://gravity.com},
	journal = {{GitHub}},
	author = {Goose},
	year = {2012},
	howpublished = {{https://github.com/GravityLabs/goose}}
},

@book{kleinberg2006,
	address = {Boston},
	title = {Algorithm design},
	isbn = {0321295358  9780321295354},
	abstract = {{"Algorithm} Design takes a fresh approach to the algorithms course, introducing algorithmic ideas through the real-world problems that motivate them. In a clear, direct style, Jon Kleinberg and Eva Tardos teach students to analyze and define problems for themselves, and from this to recognize which design principles are appropriate for a given situation. The text encourages a greater understanding of the algorithm design process and an appreciation of the role of algorithms in the broader field of computer {science."--BOOK} {JACKET.}},
	publisher = {{Pearson/Addison-Wesley}},
	author = {Kleinberg, Jon and Tardos, \'{E}va},
	year = {2006}
},

@misc{garg2012,
	title = {String Matching Algorithms and their Applicability in various Applications},
	url = {http://www.academia.edu/1975650/String_Matching_Algorithms_and_their_Applicability_in_various_Applications},
	abstract = {In this paper the applicability of the various strings matching algorithms are being described. Which algorithm is best in which application and why. This describes the optimal algorithm for various activities that include string matching as an},
	author = {Garg, Deepak},
	year = {2012},
	keywords = {academia, academics, Biology, Chemistry, Computer Science, Earth Sciences, Economics, English, Geography, History, Law, Math, Medicine, Philosophy, Physics, Political Science, Psychology, Religion, research, universities},
	howpublished = {{http://www.academia.edu/1975650/String\_Matching\_Algorithms\_and\_their\_Applicability\_in\_various\_Applications}}
},

@inproceedings{oita2010,
	title = {Archiving Data Objects using Web Feeds},
	url = {http://hal.inria.fr/inria-00537962},
	abstract = {Web feeds, either in {RSS} or Atom {XML-based} formats, are evolving descriptive documents that characterize a dynamic hub of a Web site and help subscribers keep up with what is the most recent Web content of interest. In this paper, we show how Web feeds can be useful instruments for information extraction and Web page change detection. Web pages referenced by feed items are usually blog posts or news articles, data with a dynamic (then ephemeral) nature and which is clustered topically in a feed channel. We monitor Web channels and extract from the associated Web pages the text and references corresponding to Web articles. The result is enriched with the timestamp and additional metadata mined from the feed, and encapsulated in a 'data object'. The data object will be in particular information devoided of all the template elements or advertisements. These irrelevant elements, generically called boileplate, are not only consuming time and space from the crawler's point of view, but also hinder the data analysis process. We first make some statistics on a set of Web feeds, by crawling them for a period of time and observing their temporal aspects. Then we present the algorithm used for article extraction, algorithm that uses the feed semantics (more specifically the description and title of feed items) in order to identify the {DOM} node in the {HTML} page that contains the article. The data objects constructed in this way can be used as a semantic overlay collection for an archive or in the context of an incremental crawl, making it more efficient by detecting change at data object level. Experiments on the extraction technique are done in order to validate our approach, with good results even in cases when other techniques fail. We finally discuss useful applications based on the extraction and change detection of Web objects.},
	author = {Oita, Marilena and Senellart, Pierre},
	month = sep,
	year = {2010},
	keywords = {Web archiving data object Web feed Web page dynamics}
},

@article{zhai2007,
	title = {Extracting Web Data Using {Instance-Based} Learning},
	volume = {10},
	issn = {{1386-145X}},
	url = {http://dx.doi.org/10.1007/s11280-007-0022-0},
	doi = {10.1007/s11280-007-0022-0},
	abstract = {This paper studies structured data extraction from Web pages. Existing approaches to data extraction include wrapper induction and automated methods. In this paper, we propose an instance-based learning method, which performs extraction by comparing each new instance to be extracted with labeled instances. The key advantage of our method is that it does not require an initial set of labeled pages to learn extraction rules as in wrapper induction. Instead, the algorithm is able to start extraction from a single labeled instance. Only when a new instance cannot be extracted does it need labeling. This avoids unnecessary page labeling, which solves a major problem with inductive learning (or wrapper induction), i.e., the set of labeled instances may not be representative of all other instances. The instance-based approach is very natural because structured data on the Web usually follow some fixed templates. Pages of the same template usually can be extracted based on a single page instance of the template. A novel technique is proposed to match a new instance with a manually labeled instance and in the process to extract the required data items from the new instance. The technique is also very efficient. Experimental results based on 1,200 pages from 24 diverse Web sites demonstrate the effectiveness of the method. It also outperforms the state-of-the-art existing systems significantly.},
	number = {2},
	journal = {World Wide Web},
	author = {Zhai, Yanhong and Liu, Bing},
	month = jun,
	year = {2007},
	keywords = {instance-based learning, web content mining, web data extraction},
	pages = {113{\textendash}132}
},

@article{dice1945,
	title = {Measures of the Amount of Ecologic Association Between Species},
	volume = {26},
	issn = {00129658},
	url = {http://www.jstor.org/discover/10.2307/1932409?uid=16673648&uid=3737760&uid=2&uid=3&uid=5911624&uid=67&uid=62&uid=16673552&sid=21103011050973},
	doi = {10.2307/1932409},
	number = {3},
	journal = {Ecology},
	author = {Dice, Lee R.},
	month = jul,
	year = {1945},
	pages = {297}
},

@inproceedings{caffaro????,
	title = {Invenio: A Modern Digital Library for Grey Literature},
	booktitle = {Twelfth International Conference on Grey Literature},
	author = {Caffaro, J. and Kaplun, S.}
},

@inproceedings{kordomatis2013,
	address = {New York, {NY}, {USA}},
	series = {{WIMS} '13},
	title = {Web object identification for web automation and meta-search},
	isbn = {978-1-4503-1850-1},
	url = {http://doi.acm.org/10.1145/2479787.2479798},
	doi = {10.1145/2479787.2479798},
	abstract = {Web object identification plays an important role in research fields such as information extraction, web automation, and web form understanding for building meta-search engines. In contrast to other works, we approach this problem by analyzing various spatial, visual, functional and textual characteristics of web pages. We compute 49 unique features for all visible web page elements, which are then applied to machine learning classifiers in order to identify similar elements on other previously unexamined web pages. We evaluate our approach with different scenarios by analyzing the relevance of the chosen features and the classification rate of the applied classifiers. These scenarios focus on understanding search forms from the transportation domain, particularly flight, train, and bus connections. The results of the evaluation are very promising.},
	booktitle = {Proceedings of the 3rd International Conference on Web Intelligence, Mining and Semantics},
	publisher = {{ACM}},
	author = {Kordomatis, Iraklis and Herzog, Christoph and Fayzrakhmanov, Ruslan R. and {Kr\"{u}pl-Sypien}, Bernhard and Holzinger, Wolfgang and Baumgartner, Robert},
	year = {2013},
	keywords = {machine learning, web accessibility, web automation, web object identification, web page visual representation},
	pages = {13:1{\textendash}13:12}
},

@misc{livefyre2014,
	url = {http://web.livefyre.com/},
	journal = {Livefyre},
	author = {Livefyre},
	year = {2014},
	howpublished = {http://web.livefyre.com/}
},

@inproceedings{burton2011,
	title = {The {ICWSM} 2011 Spinn3r Dataset},
	booktitle = {Fifth Annual Conference on Weblogs and Social Media},
	author = {Burton, K. and Kasch, N. and Soboroff, I.},
	year = {2011}
},

@misc{brianwilson2008-a,
	title = {Metadata Analysis and Mining Application: {W3C} validator research},
	url = {http://dev.opera.com/articles/view/mama-w3c-validator-research-2/},
	author = {Brian Wilson},
	year = {2008},
	howpublished = {http://dev.opera.com/articles/view/mama-w3c-validator-research-2/}
},

@inproceedings{hundt2011,
	title = {Loop Recognition in {C++/Java/Go/Scala}},
	url = {https://days2011.scala-lang.org/sites/days2011/files/ws3-1-Hundt.pdf},
	booktitle = {Proceedings of Scala Days},
	author = {Hundt, Robert},
	year = {2011}
},

@misc{twisted2014,
	url = {http://twistedmatrix.com/},
	author = {Twisted},
	year = {2014},
	howpublished = {http://twistedmatrix.com/}
},

@inproceedings{faheem2012,
	address = {New York, {NY}, {USA}},
	series = {{WWW} '12 Companion},
	title = {Intelligent crawling of web applications for web archiving},
	isbn = {978-1-4503-1230-1},
	url = {http://doi.acm.org/10.1145/2187980.2187996},
	doi = {10.1145/2187980.2187996},
	abstract = {The steady growth of the World Wide Web raises challenges regarding the preservation of meaningful Web data. Tools used currently by Web archivists blindly crawl and store Web pages found while crawling, disregarding the kind of Web site currently accessed (which leads to suboptimal crawling strategies) and whatever structured content is contained in Web pages (which results in page-level archives whose content is hard to exploit). We focus in this {PhD} work on the crawling and archiving of publicly accessible Web applications, especially those of the social Web. A Web application is any application that uses Web standards such as {HTML} and {HTTP} to publish information on the Web, accessible by Web browsers. Examples include Web forums, social networks, geolocation services, etc. We claim that the best strategy to crawl these applications is to make the Web crawler aware of the kind of application currently processed, allowing it to refine the list of {URLs} to process, and to annotate the archive with information about the structure of crawled content. We add adaptive characteristics to an archival Web crawler: being able to identify when a Web page belongs to a given Web application and applying the appropriate crawling and content extraction methodology.},
	booktitle = {Proceedings of the 21st international conference companion on World Wide Web},
	publisher = {{ACM}},
	author = {Faheem, Muhammad},
	year = {2012},
	keywords = {archiving, Crawling, extraction, web application, {XPath}},
	pages = {127{\textendash}132}
},

@misc{python-readability2011,
	url = {https://github.com/gfxmonk/python-readability},
	abstract = {python-readability - python port of arc90's readability bookmarklet},
	journal = {{GitHub}},
	author = {python-readability},
	year = {2011},
	howpublished = {https://github.com/gfxmonk/python-readability}
},

@inproceedings{shkapenyuk2002,
	title = {Design and implementation of a high-performance distributed Web crawler},
	doi = {10.1109/ICDE.2002.994750},
	abstract = {Broad Web search engines as well as many more specialized search tools rely on Web crawlers to acquire large collections of pages for indexing and analysis. Such a Web crawler may interact with millions of hosts over a period of weeks or months, and thus issues of robustness, flexibility, and manageability are of major importance. In addition, {I/O} performance, network resources, and {OS} limits must be taken into account in order to achieve high performance at a reasonable cost. In this paper, we describe the design and implementation of a distributed Web crawler that runs on a network of workstations. The crawler scales to (at least) several hundred pages per second, is resilient against system crashes and other events, and can be adapted to various crawling applications. We present the software architecture of the system, discuss the, performance bottlenecks, and describe efficient techniques for achieving high performance. We also report preliminary experimental results based on a crawl of 120 million pages on 5 million hosts},
	booktitle = {18th International Conference on Data Engineering, 2002. Proceedings},
	author = {Shkapenyuk, V. and Suel, Torsten},
	year = {2002},
	keywords = {analysis, Application software, broad Web search engines, Computer crashes, Costs, Crawlers, flexibility, high-performance distributed Web crawler, hypermedia, indexing, information retrieval, Internet, {I/O} performance, manageability, network of workstations, network resources, {OS} limits, performance bottlenecks, robustness, search engines, software architecture, specialized search tools, Web search, workstation clusters, Workstations},
	pages = {357--368}
},

@inproceedings{boldi2003,
	title = {{UbiCrawler:} a scalable fully distributed web crawler},
	shorttitle = {{UbiCrawler}},
	url = {http://citeseer.uark.edu:8380/citeseerx/viewdoc/similar;jsessionid=9A1EB49FA584CEE26215545EDBE977AA?doi=10.1.1.14.4239&type=sc},
	abstract = {{CiteSeerX} - Document Details {(Isaac} Councill, Lee Giles): We report our experience in implementing {UbiCrawler}, a scalable distributed web crawler, using the Java programming language. The main features of {UbiCrawler} are platform independence, fault tolerance, a very effective assignment function for partitioning the domain to crawl, and more in general the complete decentralization of every task. The necessity of handling very large sets of data has highlighted some limitation of the Java {APIs}, which prompted the authors to partially reimplement them.},
	author = {Boldi, Paolo and Codenotti, Bruno and Santini, Massimo and Vigna, Sebastiano},
	year = {2003},
	keywords = {Bruno Codenotti, {CiteSeerX}, Massimo Santini, Paolo Boldi, Sebastiano Vigna}
},

@misc{scrapy2014,
	url = {http://scrapy.org/},
	abstract = {Scrapy, a fast high-level screen scraping and web crawling framework for Python.},
	author = {Scrapy},
	year = {2014},
	howpublished = {http://scrapy.org/}
},

@techreport{ferrara2012,
	type = {{arXiv} e-print},
	title = {Web Data Extraction, Applications and Techniques: A Survey},
	shorttitle = {Web Data Extraction, Applications and Techniques},
	url = {http://arxiv.org/abs/1207.0246},
	abstract = {Web Data Extraction is an important problem that has been studied by means of different scientific tools and in a broad range of application domains. Many approaches to extracting data from the Web have been designed to solve specific problems and operate in ad-hoc application domains. Other approaches, instead, heavily reuse techniques and algorithms developed in the field of Information Extraction. This survey aims at providing a structured and comprehensive overview of the research efforts made in the field of Web Data Extraction. The fil rouge of our work is to provide a classification of existing approaches in terms of the applications for which they have been employed. This differentiates our work from other surveys devoted to classify existing approaches on the basis of the algorithms, techniques and tools they use. We classified Web Data Extraction approaches into categories and, for each category, we illustrated the basic techniques along with their main variants. We grouped existing applications in two main areas: applications at the Enterprise level and at the Social Web level. Such a classification relies on a twofold reason: on one hand, Web Data Extraction techniques emerged as a key tool to perform data analysis in Business and Competitive Intelligence systems as well as for business process re-engineering. On the other hand, Web Data Extraction techniques allow for gathering a large amount of structured data continuously generated and disseminated by Web 2.0, Social Media and Online Social Network users and this offers unprecedented opportunities of analyzing human behaviors on a large scale. We discussed also about the potential of cross-fertilization, i.e., on the possibility of re-using Web Data Extraction techniques originally designed to work in a given domain, in other domains.},
	number = {1207.0246},
	author = {Ferrara, Emilio and De Meo, Pasquale and Fiumara, Giacomo and Baumgartner, Robert},
	month = jul,
	year = {2012},
	keywords = {Computer Science - Information Retrieval}
},

@article{levenshtein1966,
	title = {Binary codes capable of correcting deletions, insertions and reversals.},
	volume = {10},
	abstract = {Seems to be the first person to define what became known as the (simple) edit-distance and show it to be a metric. Paper mostly about constructing optimal codes to transmit such corrections.},
	number = {8},
	journal = {Soviet Physics Doklady.},
	author = {Levenshtein, {VI}},
	month = feb,
	year = {1966},
	keywords = {algorithm, code, codes, comparison, correcting, delete, deletion, distance, dpa, dynamic, edit, edit-distance, error, import, indel, insert, lcs, lcss, metric, programming, sellers, sequence, similarity, string, strings, string-to-string},
	pages = {707--710}
},

@misc{phantomjs2014,
	url = {http://phantomjs.org/},
	author = {{PhantomJS}},
	year = {2014},
	howpublished = {http://phantomjs.org/}
},

@article{agha1985,
	title = {{ACTORS:} A Model of Concurrent Computation in Distributed Systems},
	shorttitle = {{ACTORS}},
	url = {http://dspace.mit.edu/handle/1721.1/6952},
	abstract = {A foundational model of concurrency is  developed in this thesis. We examine issues  in the design of parallel systems and show  why the actor model is suitable for exploiting  large-scale parallelism. Concurrency in actors  is constrained only by the availability of  hardware resources and by the logical  dependence inherent in the computation.  Unlike dataflow and functional programming,  however, actors are dynamically  reconfigurable and can model shared  resources with changing local state.  Concurrency is spawned in actors using  asynchronous message-passing, pipelining,  and the dynamic creation of actors. This  thesis deals with some central issues in  distributed computing. Specifically, problems  of divergence and deadlock are addressed.  For example, actors permit dynamic deadlock  detection and removal. The problem of  divergence is contained because  independent transactions can execute  concurrently and potentially infinite processes  are nevertheless available for interaction.},
	author = {Agha, Gul Abdulnabi},
	month = jun,
	year = {1985}
},

@book{hohpe2003,
	address = {Boston, {MA}, {USA}},
	title = {Enterprise Integration Patterns: Designing, Building, and Deploying Messaging Solutions},
	isbn = {0321200683},
	shorttitle = {Enterprise Integration Patterns},
	abstract = {Would you like to use a consistent visual notation for drawing integration solutions? Look inside the front cover. Do you want to harness the power of asynchronous systems without getting caught in the pitfalls? See {"Thinking} Asynchronously" in the Introduction. Do you want to know which style of application integration is best for your purposes? See Chapter 2, Integration Styles. Do you want to learn techniques for processing messages concurrently? See Chapter 10, Competing Consumers and Message Dispatcher. Do you want to learn how you can track asynchronous messages as they flow across distributed systems? See Chapter 11, Message History and Message Store. Do you want to understand how a system designed using integration patterns can be implemented using Java Web services, {.NET} message queuing, and a {TIBCO-based} publish-subscribe architecture? See Chapter 9, Interlude: Composed {Messaging.Utilizing} years of practical experience, seasoned experts Gregor Hohpe and Bobby Woolf show how asynchronous messaging has proven to be the best strategy for enterprise integration success. However, building and deploying messaging solutions presents a number of problems for developers. Enterprise Integration Patterns provides an invaluable catalog of sixty-five patterns, with real-world solutions that demonstrate the formidable of messaging and help you to design effective messaging solutions for your {enterprise.The} authors also include examples covering a variety of different integration technologies, such as {JMS}, {MSMQ}, {TIBCO} {ActiveEnterprise}, Microsoft {BizTalk}, {SOAP}, and {XSL.} A case study describing a bond trading system illustrates the patterns in practice, and the book offers a look at emerging standards, as well as insights into what the future of enterprise integration might {hold.This} book provides a consistent vocabulary and visual notation framework to describe large-scale integration solutions across many technologies. It also explores in detail the advantages and limitations of asynchronous messaging architectures. The authors present practical advice on designing code that connects an application to a messaging system, and provide extensive information to help you determine when to send a message, how to route it to the proper destination, and how to monitor the health of a messaging system. If you want to know how to manage, monitor, and maintain a messaging system once it is in use, get this book. {0321200683B09122003}},
	publisher = {{Addison-Wesley} Longman Publishing Co., Inc.},
	author = {Hohpe, Gregor and Woolf, Bobby},
	year = {2003}
},

@article{navarro2001,
	title = {A Guided Tour to Approximate String Matching},
	volume = {33},
	issn = {0360-0300},
	url = {http://doi.acm.org/10.1145/375360.375365},
	doi = {10.1145/375360.375365},
	abstract = {We survey the current techniques to cope with the problem of string matching that allows errors. This is becoming a more and more relevant issue for many fast growing areas such as information retrieval and computational biology. We focus on online searching and mostly on edit distance, explaining the problem and its relevance, its statistical behavior, its history and current developments, and the central ideas of the algorithms and their complexities. We present a number of experiments to compare the performance of the different algorithms and show which are the best choices. We conclude with some directions for future work and open problems.},
	number = {1},
	journal = {{ACM} Comput. Surv.},
	author = {Navarro, Gonzalo},
	month = mar,
	year = {2001},
	keywords = {edit distance, Levenshtein distance, online string matching, text searching allowing errors},
	pages = {31{\textendash}88}
}
