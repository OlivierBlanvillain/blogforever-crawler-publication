
@article{furche2013,
	title = {{OXPath:} A language for scalable data extraction, automation, and crawling on the deep web},
	volume = {22},
	issn = {1066-8888},
	shorttitle = {{OXPath}},
	url = {http://dx.doi.org/10.1007/s00778-012-0286-6},
	doi = {10.1007/s00778-012-0286-6},
	abstract = {The evolution of the web has outpaced itself: A growing wealth of information and increasingly sophisticated interfaces necessitate automated processing, yet existing automation and data extraction technologies have been overwhelmed by this very growth. To address this trend, we identify four key requirements for web data extraction, automation, and (focused) web crawling: (1) interact with sophisticated web application interfaces, (2) precisely capture the relevant data to be extracted, (3) scale with the number of visited pages, and (4) readily embed into existing web technologies. We introduce {OXPath} as an extension of {XPath} for interacting with web applications and extracting data thus revealed--matching all the above requirements. {OXPath's} page-at-a-time evaluation guarantees memory use independent of the number of visited pages, yet remains polynomial in time. We experimentally validate the theoretical complexity and demonstrate that {OXPath's} resource consumption is dominated by page rendering in the underlying browser. With an extensive study of sublanguages and properties of {OXPath}, we pinpoint the effect of specific features on evaluation performance. Our experiments show that {OXPath} outperforms existing commercial and academic data extraction tools by a wide margin.},
	number = {1},
	journal = {The {VLDB} Journal},
	author = {Furche, Tim and Gottlob, Georg and Grasso, Giovanni and Schallhart, Christian and Sellers, Andrew},
	month = feb,
	year = {2013},
	keywords = {{AJAX}, Automation, Crawling, Data extraction, {DOM}, Web applications, Web extraction, {XPath}},
	pages = {47{\textendash}72}
},

@inproceedings{oita2010,
	title = {Archiving Data Objects using Web Feeds},
	url = {http://hal.inria.fr/inria-00537962},
	abstract = {Web feeds, either in {RSS} or Atom {XML-based} formats, are evolving descriptive documents that characterize a dynamic hub of a Web site and help subscribers keep up with what is the most recent Web content of interest. In this paper, we show how Web feeds can be useful instruments for information extraction and Web page change detection. Web pages referenced by feed items are usually blog posts or news articles, data with a dynamic (then ephemeral) nature and which is clustered topically in a feed channel. We monitor Web channels and extract from the associated Web pages the text and references corresponding to Web articles. The result is enriched with the timestamp and additional metadata mined from the feed, and encapsulated in a 'data object'. The data object will be in particular information devoided of all the template elements or advertisements. These irrelevant elements, generically called boileplate, are not only consuming time and space from the crawler's point of view, but also hinder the data analysis process. We first make some statistics on a set of Web feeds, by crawling them for a period of time and observing their temporal aspects. Then we present the algorithm used for article extraction, algorithm that uses the feed semantics (more specifically the description and title of feed items) in order to identify the {DOM} node in the {HTML} page that contains the article. The data objects constructed in this way can be used as a semantic overlay collection for an archive or in the context of an incremental crawl, making it more efficient by detecting change at data object level. Experiments on the extraction technique are done in order to validate our approach, with good results even in cases when other techniques fail. We finally discuss useful applications based on the extraction and change detection of Web objects.},
	author = {Oita, Marilena and Senellart, Pierre},
	month = sep,
	year = {2010},
	keywords = {Web archiving data object Web feed Web page dynamics}
},

@inproceedings{caffaro????,
	title = {Invenio: A Modern Digital Library for Grey Literature},
	booktitle = {Twelfth International Conference on Grey Literature},
	author = {Caffaro, J. and Kaplun, S.}
},

@inproceedings{kordomatis2013,
	address = {New York, {NY}, {USA}},
	series = {{WIMS} '13},
	title = {Web object identification for web automation and meta-search},
	isbn = {978-1-4503-1850-1},
	url = {http://doi.acm.org/10.1145/2479787.2479798},
	doi = {10.1145/2479787.2479798},
	abstract = {Web object identification plays an important role in research fields such as information extraction, web automation, and web form understanding for building meta-search engines. In contrast to other works, we approach this problem by analyzing various spatial, visual, functional and textual characteristics of web pages. We compute 49 unique features for all visible web page elements, which are then applied to machine learning classifiers in order to identify similar elements on other previously unexamined web pages. We evaluate our approach with different scenarios by analyzing the relevance of the chosen features and the classification rate of the applied classifiers. These scenarios focus on understanding search forms from the transportation domain, particularly flight, train, and bus connections. The results of the evaluation are very promising.},
	booktitle = {Proceedings of the 3rd International Conference on Web Intelligence, Mining and Semantics},
	publisher = {{ACM}},
	author = {Kordomatis, Iraklis and Herzog, Christoph and Fayzrakhmanov, Ruslan R. and {Kr\"{u}pl-Sypien}, Bernhard and Holzinger, Wolfgang and Baumgartner, Robert},
	year = {2013},
	keywords = {machine learning, web accessibility, web automation, web object identification, web page visual representation},
	pages = {13:1{\textendash}13:12}
},

@misc{????,
	title = {Home},
	url = {http://web.livefyre.com/},
	journal = {Livefyre},
	howpublished = {http://web.livefyre.com/}
},

@inproceedings{burton????,
	title = {The {ICWSM} 2011 Spinn3r Dataset},
	booktitle = {Fifth Annual Conference on Weblogs and Social Media {(ICWSM} 2011)},
	author = {Burton, K. and Kasch, N. and Soboroff, I.}
},

@inproceedings{hundt2011,
	title = {Loop Recognition in {C++/Java/Go/Scala}},
	url = {https://days2011.scala-lang.org/sites/days2011/files/ws3-1-Hundt.pdf},
	booktitle = {Proceedings of Scala Days 2011},
	author = {Hundt, Robert},
	year = {2011}
},

@inproceedings{faheem2012,
	address = {New York, {NY}, {USA}},
	series = {{WWW} '12 Companion},
	title = {Intelligent crawling of web applications for web archiving},
	isbn = {978-1-4503-1230-1},
	url = {http://doi.acm.org/10.1145/2187980.2187996},
	doi = {10.1145/2187980.2187996},
	abstract = {The steady growth of the World Wide Web raises challenges regarding the preservation of meaningful Web data. Tools used currently by Web archivists blindly crawl and store Web pages found while crawling, disregarding the kind of Web site currently accessed (which leads to suboptimal crawling strategies) and whatever structured content is contained in Web pages (which results in page-level archives whose content is hard to exploit). We focus in this {PhD} work on the crawling and archiving of publicly accessible Web applications, especially those of the social Web. A Web application is any application that uses Web standards such as {HTML} and {HTTP} to publish information on the Web, accessible by Web browsers. Examples include Web forums, social networks, geolocation services, etc. We claim that the best strategy to crawl these applications is to make the Web crawler aware of the kind of application currently processed, allowing it to refine the list of {URLs} to process, and to annotate the archive with information about the structure of crawled content. We add adaptive characteristics to an archival Web crawler: being able to identify when a Web page belongs to a given Web application and applying the appropriate crawling and content extraction methodology.},
	booktitle = {Proceedings of the 21st international conference companion on World Wide Web},
	publisher = {{ACM}},
	author = {Faheem, Muhammad},
	year = {2012},
	keywords = {archiving, Crawling, extraction, web application, {XPath}},
	pages = {127{\textendash}132}
},

@inproceedings{gkotsis2013,
	address = {Republic and Canton of Geneva, Switzerland},
	series = {{WWW} '13 Companion},
	title = {Zero-cost labelling with web feeds for weblog data extraction},
	isbn = {978-1-4503-2038-2},
	url = {http://dl.acm.org/citation.cfm?id=2487788.2487819},
	abstract = {Data extraction from web pages often involves either human intervention for training a wrapper or a reduced level of granularity in the information acquired. Even though the study of social media has drawn the attention of researchers, weblogs remain a part of the web that cannot be harvested efficiently. In this paper, we propose a fully automated approach in generating a wrapper for weblogs, which exploits web feeds for cheap labelling of weblog properties. Instead of performing a pairwise comparison between posts, the model matches the values of the web feeds against their corresponding {HTML} elements retrieved from multiple weblog posts. It adopts a probabilistic approach for deriving a set of rules and automating the process of wrapper generation. Our evaluation shows that our approach is robust, accurate and efficient in handling different types of weblogs.},
	booktitle = {Proceedings of the 22nd international conference on World Wide Web companion},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Gkotsis, George and Stepanyan, Karen and Cristea, Alexandra I. and Joy, Mike S.},
	year = {2013},
	keywords = {Data extraction, weblogs, wrapper induction},
	pages = {73{\textendash}74}
},

@techreport{ferrara2012,
	type = {{arXiv} e-print},
	title = {Web Data Extraction, Applications and Techniques: A Survey},
	shorttitle = {Web Data Extraction, Applications and Techniques},
	url = {http://arxiv.org/abs/1207.0246},
	abstract = {Web Data Extraction is an important problem that has been studied by means of different scientific tools and in a broad range of application domains. Many approaches to extracting data from the Web have been designed to solve specific problems and operate in ad-hoc application domains. Other approaches, instead, heavily reuse techniques and algorithms developed in the field of Information Extraction. This survey aims at providing a structured and comprehensive overview of the research efforts made in the field of Web Data Extraction. The fil rouge of our work is to provide a classification of existing approaches in terms of the applications for which they have been employed. This differentiates our work from other surveys devoted to classify existing approaches on the basis of the algorithms, techniques and tools they use. We classified Web Data Extraction approaches into categories and, for each category, we illustrated the basic techniques along with their main variants. We grouped existing applications in two main areas: applications at the Enterprise level and at the Social Web level. Such a classification relies on a twofold reason: on one hand, Web Data Extraction techniques emerged as a key tool to perform data analysis in Business and Competitive Intelligence systems as well as for business process re-engineering. On the other hand, Web Data Extraction techniques allow for gathering a large amount of structured data continuously generated and disseminated by Web 2.0, Social Media and Online Social Network users and this offers unprecedented opportunities of analyzing human behaviors on a large scale. We discussed also about the potential of cross-fertilization, i.e., on the possibility of re-using Web Data Extraction techniques originally designed to work in a given domain, in other domains.},
	number = {1207.0246},
	author = {Ferrara, Emilio and De Meo, Pasquale and Fiumara, Giacomo and Baumgartner, Robert},
	month = jul,
	year = {2012},
	keywords = {Computer Science - Information Retrieval}
}