
@misc{2013,
	title = {Web crawler},
	copyright = {Creative Commons {Attribution-ShareAlike} License},
	url = {http://en.wikipedia.org/wiki/Web_crawler#Architectures},
	abstract = {A Web crawler is an Internet bot that systematically browses the World Wide Web, typically for the purpose of Web indexing.},
	journal = {Wikipedia, the free encyclopedia},
	month = nov,
	year = {2013},
	note = {Page Version {ID:} 581223216}
},

@misc{antinharasymiv2011,
	title = {Blogger Dynamic Views},
	url = {http://goo.gl/Ma3G2I},
	author = {Antin Harasymiv},
	year = {2011},
	howpublished = {{http://goo.gl/Ma3G2I}}
},

@inproceedings{muslea1999,
	address = {New York, {NY}, {USA}},
	series = {{AGENTS} '99},
	title = {A hierarchical approach to wrapper induction},
	isbn = {{1-58113-066-X}},
	url = {http://doi.acm.org/10.1145/301136.301191},
	doi = {10.1145/301136.301191},
	booktitle = {Proceedings of the third annual conference on Autonomous Agents},
	publisher = {{ACM}},
	author = {Muslea, Ion and Minton, Steve and Knoblock, Craig},
	year = {1999},
	pages = {190{\textendash}197}
},

@misc{thereactivemanifesto2013,
	url = {http://www.reactivemanifesto.org/},
	author = {The Reactive Manifesto},
	year = {2013},
	howpublished = {http://www.reactivemanifesto.org/}
},

@article{furche2013,
	title = {{OXPath:} A language for scalable data extraction, automation, and crawling on the deep web},
	volume = {22},
	issn = {1066-8888},
	shorttitle = {{OXPath}},
	url = {http://dx.doi.org/10.1007/s00778-012-0286-6},
	doi = {10.1007/s00778-012-0286-6},
	abstract = {The evolution of the web has outpaced itself: A growing wealth of information and increasingly sophisticated interfaces necessitate automated processing, yet existing automation and data extraction technologies have been overwhelmed by this very growth. To address this trend, we identify four key requirements for web data extraction, automation, and (focused) web crawling: (1) interact with sophisticated web application interfaces, (2) precisely capture the relevant data to be extracted, (3) scale with the number of visited pages, and (4) readily embed into existing web technologies. We introduce {OXPath} as an extension of {XPath} for interacting with web applications and extracting data thus revealed--matching all the above requirements. {OXPath's} page-at-a-time evaluation guarantees memory use independent of the number of visited pages, yet remains polynomial in time. We experimentally validate the theoretical complexity and demonstrate that {OXPath's} resource consumption is dominated by page rendering in the underlying browser. With an extensive study of sublanguages and properties of {OXPath}, we pinpoint the effect of specific features on evaluation performance. Our experiments show that {OXPath} outperforms existing commercial and academic data extraction tools by a wide margin.},
	number = {1},
	journal = {The {VLDB} Journal},
	author = {Furche, Tim and Gottlob, Georg and Grasso, Giovanni and Schallhart, Christian and Sellers, Andrew},
	month = feb,
	year = {2013},
	keywords = {{AJAX}, Automation, Crawling, Data extraction, {DOM}, Web applications, Web extraction, {XPath}},
	pages = {47{\textendash}72}
},

@inproceedings{oita2010,
	title = {Archiving Data Objects using Web Feeds},
	url = {http://hal.inria.fr/inria-00537962},
	abstract = {Web feeds, either in {RSS} or Atom {XML-based} formats, are evolving descriptive documents that characterize a dynamic hub of a Web site and help subscribers keep up with what is the most recent Web content of interest. In this paper, we show how Web feeds can be useful instruments for information extraction and Web page change detection. Web pages referenced by feed items are usually blog posts or news articles, data with a dynamic (then ephemeral) nature and which is clustered topically in a feed channel. We monitor Web channels and extract from the associated Web pages the text and references corresponding to Web articles. The result is enriched with the timestamp and additional metadata mined from the feed, and encapsulated in a 'data object'. The data object will be in particular information devoided of all the template elements or advertisements. These irrelevant elements, generically called boileplate, are not only consuming time and space from the crawler's point of view, but also hinder the data analysis process. We first make some statistics on a set of Web feeds, by crawling them for a period of time and observing their temporal aspects. Then we present the algorithm used for article extraction, algorithm that uses the feed semantics (more specifically the description and title of feed items) in order to identify the {DOM} node in the {HTML} page that contains the article. The data objects constructed in this way can be used as a semantic overlay collection for an archive or in the context of an incremental crawl, making it more efficient by detecting change at data object level. Experiments on the extraction technique are done in order to validate our approach, with good results even in cases when other techniques fail. We finally discuss useful applications based on the extraction and change detection of Web objects.},
	author = {Oita, Marilena and Senellart, Pierre},
	month = sep,
	year = {2010},
	keywords = {Web archiving data object Web feed Web page dynamics}
},

@article{zhai2007,
	title = {Extracting Web Data Using {Instance-Based} Learning},
	volume = {10},
	issn = {{1386-145X}},
	url = {http://dx.doi.org/10.1007/s11280-007-0022-0},
	doi = {10.1007/s11280-007-0022-0},
	abstract = {This paper studies structured data extraction from Web pages. Existing approaches to data extraction include wrapper induction and automated methods. In this paper, we propose an instance-based learning method, which performs extraction by comparing each new instance to be extracted with labeled instances. The key advantage of our method is that it does not require an initial set of labeled pages to learn extraction rules as in wrapper induction. Instead, the algorithm is able to start extraction from a single labeled instance. Only when a new instance cannot be extracted does it need labeling. This avoids unnecessary page labeling, which solves a major problem with inductive learning (or wrapper induction), i.e., the set of labeled instances may not be representative of all other instances. The instance-based approach is very natural because structured data on the Web usually follow some fixed templates. Pages of the same template usually can be extracted based on a single page instance of the template. A novel technique is proposed to match a new instance with a manually labeled instance and in the process to extract the required data items from the new instance. The technique is also very efficient. Experimental results based on 1,200 pages from 24 diverse Web sites demonstrate the effectiveness of the method. It also outperforms the state-of-the-art existing systems significantly.},
	number = {2},
	journal = {World Wide Web},
	author = {Zhai, Yanhong and Liu, Bing},
	month = jun,
	year = {2007},
	keywords = {instance-based learning, web content mining, web data extraction},
	pages = {113{\textendash}132}
},

@inproceedings{caffaro????,
	title = {Invenio: A Modern Digital Library for Grey Literature},
	booktitle = {Twelfth International Conference on Grey Literature},
	author = {Caffaro, J. and Kaplun, S.}
},

@inproceedings{kordomatis2013,
	address = {New York, {NY}, {USA}},
	series = {{WIMS} '13},
	title = {Web object identification for web automation and meta-search},
	isbn = {978-1-4503-1850-1},
	url = {http://doi.acm.org/10.1145/2479787.2479798},
	doi = {10.1145/2479787.2479798},
	abstract = {Web object identification plays an important role in research fields such as information extraction, web automation, and web form understanding for building meta-search engines. In contrast to other works, we approach this problem by analyzing various spatial, visual, functional and textual characteristics of web pages. We compute 49 unique features for all visible web page elements, which are then applied to machine learning classifiers in order to identify similar elements on other previously unexamined web pages. We evaluate our approach with different scenarios by analyzing the relevance of the chosen features and the classification rate of the applied classifiers. These scenarios focus on understanding search forms from the transportation domain, particularly flight, train, and bus connections. The results of the evaluation are very promising.},
	booktitle = {Proceedings of the 3rd International Conference on Web Intelligence, Mining and Semantics},
	publisher = {{ACM}},
	author = {Kordomatis, Iraklis and Herzog, Christoph and Fayzrakhmanov, Ruslan R. and {Kr\"{u}pl-Sypien}, Bernhard and Holzinger, Wolfgang and Baumgartner, Robert},
	year = {2013},
	keywords = {machine learning, web accessibility, web automation, web object identification, web page visual representation},
	pages = {13:1{\textendash}13:12}
},

@misc{livefyre2013,
	url = {http://web.livefyre.com/},
	journal = {Livefyre},
	author = {Livefyre},
	year = {2013},
	howpublished = {http://web.livefyre.com/}
},

@inproceedings{burton????,
	title = {The {ICWSM} 2011 Spinn3r Dataset},
	booktitle = {Fifth Annual Conference on Weblogs and Social Media {(ICWSM} 2011)},
	author = {Burton, K. and Kasch, N. and Soboroff, I.}
},

@misc{disqus2013,
	url = {http://disqus.com/websites/},
	author = {Disqus},
	year = {2013},
	howpublished = {http://disqus.com/websites/}
},

@inproceedings{hundt2011,
	title = {Loop Recognition in {C++/Java/Go/Scala}},
	url = {https://days2011.scala-lang.org/sites/days2011/files/ws3-1-Hundt.pdf},
	booktitle = {Proceedings of Scala Days},
	author = {Hundt, Robert},
	year = {2011}
},

@misc{twisted2013,
	url = {http://twistedmatrix.com/},
	author = {Twisted},
	year = {2013},
	howpublished = {http://twistedmatrix.com/}
},

@inproceedings{faheem2012,
	address = {New York, {NY}, {USA}},
	series = {{WWW} '12 Companion},
	title = {Intelligent crawling of web applications for web archiving},
	isbn = {978-1-4503-1230-1},
	url = {http://doi.acm.org/10.1145/2187980.2187996},
	doi = {10.1145/2187980.2187996},
	abstract = {The steady growth of the World Wide Web raises challenges regarding the preservation of meaningful Web data. Tools used currently by Web archivists blindly crawl and store Web pages found while crawling, disregarding the kind of Web site currently accessed (which leads to suboptimal crawling strategies) and whatever structured content is contained in Web pages (which results in page-level archives whose content is hard to exploit). We focus in this {PhD} work on the crawling and archiving of publicly accessible Web applications, especially those of the social Web. A Web application is any application that uses Web standards such as {HTML} and {HTTP} to publish information on the Web, accessible by Web browsers. Examples include Web forums, social networks, geolocation services, etc. We claim that the best strategy to crawl these applications is to make the Web crawler aware of the kind of application currently processed, allowing it to refine the list of {URLs} to process, and to annotate the archive with information about the structure of crawled content. We add adaptive characteristics to an archival Web crawler: being able to identify when a Web page belongs to a given Web application and applying the appropriate crawling and content extraction methodology.},
	booktitle = {Proceedings of the 21st international conference companion on World Wide Web},
	publisher = {{ACM}},
	author = {Faheem, Muhammad},
	year = {2012},
	keywords = {archiving, Crawling, extraction, web application, {XPath}},
	pages = {127{\textendash}132}
},

@misc{worldwidewebconsortiumw3c2002,
	title = {Use {\textless}h1{\textgreater} for top level heading},
	url = {http://www-mit.w3.org/QA/Tips/Use_h1_for_Title},
	author = {World Wide Web Consortium {(W3C)}},
	year = {2002},
	howpublished = {{http://www-mit.w3.org/QA/Tips/Use\_h1\_for\_Title}}
},

@misc{scrapy2013,
	url = {http://scrapy.org/},
	abstract = {Scrapy, a fast high-level screen scraping and web crawling framework for Python.},
	author = {Scrapy},
	year = {2013},
	howpublished = {http://scrapy.org/}
},

@inproceedings{gkotsis2013,
	address = {Republic and Canton of Geneva, Switzerland},
	series = {{WWW} '13 Companion},
	title = {Zero-cost labelling with web feeds for weblog data extraction},
	isbn = {978-1-4503-2038-2},
	url = {http://dl.acm.org/citation.cfm?id=2487788.2487819},
	abstract = {Data extraction from web pages often involves either human intervention for training a wrapper or a reduced level of granularity in the information acquired. Even though the study of social media has drawn the attention of researchers, weblogs remain a part of the web that cannot be harvested efficiently. In this paper, we propose a fully automated approach in generating a wrapper for weblogs, which exploits web feeds for cheap labelling of weblog properties. Instead of performing a pairwise comparison between posts, the model matches the values of the web feeds against their corresponding {HTML} elements retrieved from multiple weblog posts. It adopts a probabilistic approach for deriving a set of rules and automating the process of wrapper generation. Our evaluation shows that our approach is robust, accurate and efficient in handling different types of weblogs.},
	booktitle = {Proceedings of the 22nd international conference on World Wide Web companion},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Gkotsis, George and Stepanyan, Karen and Cristea, Alexandra I. and Joy, Mike S.},
	year = {2013},
	keywords = {Data extraction, weblogs, wrapper induction},
	pages = {73{\textendash}74}
},

@techreport{ferrara2012,
	type = {{arXiv} e-print},
	title = {Web Data Extraction, Applications and Techniques: A Survey},
	shorttitle = {Web Data Extraction, Applications and Techniques},
	url = {http://arxiv.org/abs/1207.0246},
	abstract = {Web Data Extraction is an important problem that has been studied by means of different scientific tools and in a broad range of application domains. Many approaches to extracting data from the Web have been designed to solve specific problems and operate in ad-hoc application domains. Other approaches, instead, heavily reuse techniques and algorithms developed in the field of Information Extraction. This survey aims at providing a structured and comprehensive overview of the research efforts made in the field of Web Data Extraction. The fil rouge of our work is to provide a classification of existing approaches in terms of the applications for which they have been employed. This differentiates our work from other surveys devoted to classify existing approaches on the basis of the algorithms, techniques and tools they use. We classified Web Data Extraction approaches into categories and, for each category, we illustrated the basic techniques along with their main variants. We grouped existing applications in two main areas: applications at the Enterprise level and at the Social Web level. Such a classification relies on a twofold reason: on one hand, Web Data Extraction techniques emerged as a key tool to perform data analysis in Business and Competitive Intelligence systems as well as for business process re-engineering. On the other hand, Web Data Extraction techniques allow for gathering a large amount of structured data continuously generated and disseminated by Web 2.0, Social Media and Online Social Network users and this offers unprecedented opportunities of analyzing human behaviors on a large scale. We discussed also about the potential of cross-fertilization, i.e., on the possibility of re-using Web Data Extraction techniques originally designed to work in a given domain, in other domains.},
	number = {1207.0246},
	author = {Ferrara, Emilio and De Meo, Pasquale and Fiumara, Giacomo and Baumgartner, Robert},
	month = jul,
	year = {2012},
	keywords = {Computer Science - Information Retrieval}
},

@misc{phantomjs2013,
	url = {http://phantomjs.org/},
	author = {{PhantomJS}},
	year = {2013},
	howpublished = {http://phantomjs.org/}
},

@article{agha1985,
	title = {{ACTORS:} A Model of Concurrent Computation in Distributed Systems},
	shorttitle = {{ACTORS}},
	url = {http://dspace.mit.edu/handle/1721.1/6952},
	abstract = {A foundational model of concurrency is  developed in this thesis. We examine issues  in the design of parallel systems and show  why the actor model is suitable for exploiting  large-scale parallelism. Concurrency in actors  is constrained only by the availability of  hardware resources and by the logical  dependence inherent in the computation.  Unlike dataflow and functional programming,  however, actors are dynamically  reconfigurable and can model shared  resources with changing local state.  Concurrency is spawned in actors using  asynchronous message-passing, pipelining,  and the dynamic creation of actors. This  thesis deals with some central issues in  distributed computing. Specifically, problems  of divergence and deadlock are addressed.  For example, actors permit dynamic deadlock  detection and removal. The problem of  divergence is contained because  independent transactions can execute  concurrently and potentially infinite processes  are nevertheless available for interaction.},
	author = {Agha, Gul Abdulnabi},
	month = jun,
	year = {1985}
},

@book{hohpe2003,
	address = {Boston, {MA}, {USA}},
	title = {Enterprise Integration Patterns: Designing, Building, and Deploying Messaging Solutions},
	isbn = {0321200683},
	shorttitle = {Enterprise Integration Patterns},
	abstract = {Would you like to use a consistent visual notation for drawing integration solutions? Look inside the front cover. Do you want to harness the power of asynchronous systems without getting caught in the pitfalls? See {"Thinking} Asynchronously" in the Introduction. Do you want to know which style of application integration is best for your purposes? See Chapter 2, Integration Styles. Do you want to learn techniques for processing messages concurrently? See Chapter 10, Competing Consumers and Message Dispatcher. Do you want to learn how you can track asynchronous messages as they flow across distributed systems? See Chapter 11, Message History and Message Store. Do you want to understand how a system designed using integration patterns can be implemented using Java Web services, {.NET} message queuing, and a {TIBCO-based} publish-subscribe architecture? See Chapter 9, Interlude: Composed {Messaging.Utilizing} years of practical experience, seasoned experts Gregor Hohpe and Bobby Woolf show how asynchronous messaging has proven to be the best strategy for enterprise integration success. However, building and deploying messaging solutions presents a number of problems for developers. Enterprise Integration Patterns provides an invaluable catalog of sixty-five patterns, with real-world solutions that demonstrate the formidable of messaging and help you to design effective messaging solutions for your {enterprise.The} authors also include examples covering a variety of different integration technologies, such as {JMS}, {MSMQ}, {TIBCO} {ActiveEnterprise}, Microsoft {BizTalk}, {SOAP}, and {XSL.} A case study describing a bond trading system illustrates the patterns in practice, and the book offers a look at emerging standards, as well as insights into what the future of enterprise integration might {hold.This} book provides a consistent vocabulary and visual notation framework to describe large-scale integration solutions across many technologies. It also explores in detail the advantages and limitations of asynchronous messaging architectures. The authors present practical advice on designing code that connects an application to a messaging system, and provide extensive information to help you determine when to send a message, how to route it to the proper destination, and how to monitor the health of a messaging system. If you want to know how to manage, monitor, and maintain a messaging system once it is in use, get this book. {0321200683B09122003}},
	publisher = {{Addison-Wesley} Longman Publishing Co., Inc.},
	author = {Hohpe, Gregor and Woolf, Bobby},
	year = {2003}
}