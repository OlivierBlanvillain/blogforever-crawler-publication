\section{Conclusion and Future Work}
% What we presented, what we solved
In this paper, we presented the internals of the BlogForever web crawler. Its central article extraction procedure based on extraction rules generation was introduced along with theoretical and empirical evidences validating the approach. Simple adaptation of this procedure allowed to extract different types of contents, including authors, dates and comments. In order to support rapidly evolving web technologies such as JavaScript generated content, the crawler uses a web browser to render pages before processing. We also discussed the overall software architecture, highlighting the design choices made to achieve both modularity and scalability.

% Future work, hybrid algos
Future work could investigate \emph{hybrid} extraction algorithms to try to achieve near 100\% success rates. Even if overall our approach obtains better performs, there are few cases where other techniques managed to extract data where we did not. This suggests that combining our approach with others such such as word density, tree edit distance matching or even spacial reasoning could lead to better performances.

% Deployment on a distributed architecture
Another possible research direction would be the deployment of the BlogForever crawler on a large scale distributed system. This is particularly relevant in the domain of web crawling given that intensive network operations can be a serious bottleneck which benefits from the use of multiple Internet access points. We intend to explore this opportunities in our future work.

% Low level PhantomJS hooks
% I guess two future works is enough...

% Conclude the conclude
To open up further possibilities, the crawler presented in this paper is available under the MIT license from the project's website \cite{blogforevercrawler}.
