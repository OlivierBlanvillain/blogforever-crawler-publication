\section{Introduction}

Blogs disappear every day. Losing data is obviously undesirable, but even more so when this data has historic, politic or scientific value. In contrast to books, newspapers or centralized web platforms like Facebook, there is no standard method or authority to ensure blog archiving and long-term digital preservation. Yet, blogs are an important part of today's web: the WordPress reports more than 33 million new posts and 48 million new comments each month \cite{wordpress2014}. Blogs also showed to be an important resource during the 2011 Egyptian revolution by playing an instrumental role in the organization and implementation of protests. \cite{nahedeltantawy2012}. The need for preservation of this volatile communication medium is today a necessity. \comment{Do you have a better closing sentence?}

Among the challenges in developing a blog archiving software is the design of a web crawler capable of efficiently traversing blogs to harvest their content. The sheer size of the blogosphere combined with an unpredictable publishing rate of new information call for a highly scalable system, while the lack of programmatic access to the complete blog content makes the use of automatic extraction techniques necessary. The variety of available blog publishing platforms offers a limited common set of properties that a crawler can exploit, further limited\TODO{"limited" repetition is odd} by the ever-changing structure of blog contents. Finally, an increasing number of blogs heavily rely on dynamically created content to present information, using the latest web technologies, hence invalidating traditional web crawling techniques.

A key characteristic of blogs which differentiates them from regular websites is their association with web feeds. Their primary use is to provide a uniform subscription mechanism, thereby allowing users to keep track of the latest updates without the need to actually visit blogs. Concretely, a web feed is a XML file containing links to the latest blog posts along with their articles (abstract or full text) and associated metadata. Nevertheless, while web feeds essentially solve the question of update monitoring, their limited size makes it necessary to download blog web pages in order to harvest old content.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Contributions and Overview}
We present the BlogForever Crawler, a key component of the BlogForever platform responsible for traversing, extracting content and monitoring updates of blogs. Our contributions are in particular:

\begin{itemize}
  \item We present a new algorithm to build extraction rules from web feeds. We then derive an optimized reformulation tied to a particular string similarity and show that this reformulated algorithm has a linear time complexity.
  \item We show how to use this algorithm for blog article extraction and how it can be adapted to authors, publication dates and comments.
  \item nevertheless the overall crawler architecture and the specific components we implemented to efficiently traverse blogs. We explain how our design allows for both modularity and scalability.
  \item We show how we make use of a complete web browser to render JavaScript powered web pages before processing them. This step allows our crawler to effectively harvest blogs built with modern technology, such as the increasingly popular third-party commenting systems.
  \item We present and analyze performance results of our algorithm in terms of extraction success rates and execution time. A comparison is made with three state-of-the-art article extraction algorithms.
\end{itemize}

Although our crawler implementation is integrated with the BlogForever platform, the presented techniques and algorithms can be used in other applications related to Wrapper Generation and Web Data Extraction.
