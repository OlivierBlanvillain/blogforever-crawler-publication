\section{Algorithms}\label{algorithms}

This section explains in detail the algorithms we developed to extract blog posts' content and its variations for authors, dates and comments. We will see how we can take advantage of blog specific characteristics to build extraction rules applicable to all posts of a blog. We focused on minimizing the algorithmic complexity while keeping our approach simple and generic enough.

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Motivation}
% html extraction is not trival
Extracting metadata and content from HTML documents is a challenging task. Standards and format recommendations have been around for quite some time, strictly specifying how HTML documents should be organized. For instance the \code{<h1></h1>} tags should contain the highest-level heading of the page and must not appear more than once per page\cite{w3c2002}. More recently, specifications such as Microdata\cite{whatwg2013} define ways to nest semantic information and metadata inside HTML documents, but still suffer from very low usage: under 0.5\%\cite{andrewrogers2013}. In fact, the majority of websites rely on the generic \code{<span></span>} and \code{<div></div>} container elements with custom \code{id} or \code{class} attributes to organize structure pages\cite{brianwilson2008}, and more than 95\% of pages do not pass HTML validation\cite{brianwilson2008-a}. Under such circumstances, relying on HTML structure to extract content from web pages is not viable and other techniques have to be employed.

\Anotecontent{assumptions}{Our experiments on a large dataset of blogs showed than failing tests where either due to a violation of one of these assumptions, or to an insufficient amount of text in posts, such as \emph{photoblogs} with short captions.}

% assumptions when working with blogs
Having blogs as our target websites we made the following assumptions that play a central role in the extraction process\Anote{assumptions}:
\begin{enumerate}[label={(\arabic*)}]
  \item\label{havefeedAssum} Blogs provide web feeds: structured and standardized views of the most recent posts of a blog,\TODO{mention RSS and Atom, just to confirm to the reader what a web feed is?}
  \item\label{similarhtmlAssum} Posts of the same blog share a similar HTML structure.
\end{enumerate}
Web feeds usually contain about 20 posts\cite{oita2010}, often less than the total number of posts in blogs. Consequently, in order to effectively archive old content from blogs it is necessary to download and process pages beyond the ones referenced in the feed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Content extraction}
% per blog procedure, related work
To extract content, authors, dates and comments from blog posts, we proceed by first building extraction rules from the data given in the blog's web feed. These rules take advantage of our assumption that the HTML structure between different posts of the same blog is the same, in order to be applicable to all the posts of a blog. This approach has been examined in the past and has demonstrated great results (see \cite{gkotsis2013}, \cite{oita2010}), outperforming more generic extraction algorithms that work directly on HTML pages.\comment{This might be a bit too raw, give more context at this point?}

\extractionAlgo

% algorithm, inputs/output
Algorithm \ref{extractionAlgo} shows the steps we follow to build extraction rules for the content pof blog posts. As input, it takes a set of pairs of HTML pages and target contents, and finally returns an extraction rule. This rule is such that when applied to other input pages, the extracted contents are as similar as possible to the corresponding target contents.

% general idea, embarrassingly parallel
The idea is quite simple: to compute the best extraction rule for each pair of HTML page and target conent and to finally return the most frequent best rule. Making each of these computations independent might seem like a loss of information compared to alternative solutions such as having a global score for each rule, computed as the average of each rule's score over all the pages of the feed. In our experimentation we observed several cases where the independent solution performs better\TODO{example, reference or proof?}. When web feeds have many posts with very short content each, mean based solutions tend to be more influenced by these outliers and are inclined to return erronous rules which fail to return target contents but constantly have an average score.

In addition, having a independent computation for each input makes the algorithm \emph{embarrassingly parallel}\comment{it's a coined term, so it's fine, but should we have a reference here? -> Can't find a good one..}: iterations of the outer loop can trivialy be executed by multiple threads.

% input from assumptions.
Before going into further details about annex functions and time complexity, we should explain how the previously stated assumptions relate to Algorithm \ref{extractionAlgo}. Given a blog, assumption \ref{havefeedAssum} allows the crawler to obtain input for the algorithm. Indeed, web feeds provide either textual content or descriptions for their entries, as well as links to the corresponding pages. Assumption \ref{similarhtmlAssum} guaranties the existence of an appropriate an extraction rule, as well as its applicability to all the posts of a blog.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Extraction rules and string similarity}
% xpath selectors (id, else class, else path)
In our implementation, rules are queries in the XML Path Language (XPath). Consequently, standard libraries can be used parse HTML pages and apply extraction rules, thus providing the \code{Apply} function, used in Algorithm \ref{extractionAlgo}. We experimented with 3 types of XPath queries: selection over the HTML \code{id} attribute, selection over the HTML \code{class} attribute and selection with the absolute path in the HTML tree. \code{id} attributes are expected to be unique, and \code{class} attributes have showed to have better blog consistency than absolute paths. For these reasons, the \coderef{allrulesAlgo} function only returns the best rule for each node:

\allrulesAlgo

% string similarity
Unsurprisingly, the choice of the string similarity algorithm greatly influences the running time and precision of the extraction process. We chose the Sørensen–Dice coefficient similarity\cite{dice1945}, which, to the best of our knowledge\comment{maybe remove this? it weakens our choice a bit. -> at the same time it might not be the only one...}, is the only string similarity algorithm fulfilling the following criteria:

\begin{enumerate}
  \item\label{wordorderProp} Has low sensitivity to word ordering
  \item\label{lengthProp} Has low sensitivity to length variations
  \item\label{linearProp} Runs in linear time
\end{enumerate}

Properties \ref{wordorderProp} and \ref{lengthProp} are essential to deal with cases where a blog's web feed only contains an abstract or a subset of the entire post content. Table \ref{similarityTable} gives examples to illustrate how these two properties are fulfilled by the Sørensen–Dice coefficient similarity but do not hold for \emph{edit distance} based similarities such as the Levenshtein\cite{levenshtein1966} similarity.

\similarityTable

The Sørensen–Dice coefficient method operates by first building sets of pairs of adjacent characters, also knows as \emph{bigrams}, and then applying the \emph{quotient of similarity} formula:

\similarityAlgo

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Time complexity and linear reformulation}
% algo 1 is quadratic
With both the \coderef{allrulesAlgo} and \coderef{similarityAlgo} functions defined, we will show that Algorithm \ref{extractionAlgo} has a quadratic running time. First, let's assume that we have at our disposal a linear time HTML parser that constructs an appropriate data structure indexing HTML nodes on their \code{id} and \code{class} attributes, effectively making \code{Apply}~$\in$~\Oof{1}. As stated before the outer loop splits the input into independent computations and each call to \coderef{allrulesAlgo} returns (in linear time) at most as many rules as the number of nodes in its \emph{page} argument. Therefore, the body of the inner loop will be executed \Oof{n} times. Because the extraction rule can return any subtree of the queried page, each call to \coderef{similarityAlgo} takes \Oof{n}, leading to an overall quadratic running time.

% Linear reformulation
We will now present Algorithm \ref{linearAlgo}, a dynamic programming reformulation of Algorithm \ref{extractionAlgo} with linear running time. While very intuitive, the original idea of first generating extraction rules and then picking these best rules prevents us from an effective reuse of previously computed bigrams. For instance, when evaluating the extraction rule for the HTML root node, Algorithm \ref{extractionAlgo} will obtain the complete string of the page and pass it to the \coderef{similarityAlgo} function. At this point, the information on where the string could be split into substrings with already computed bigrams is not accessible, and the bigrams of the page have to be computed by linearly traversing the entire string. To overcome this limitation and implement $memoization$ \comment{without R!}over the bigram computations, Algorithm \ref{linearAlgo} uses a post-order traversal of the HTML tree to compute node bigrams from their children bigrams. This way we avoid serializing HTML subtrees for each bigram computation and we can guarantee that each character of the HTML page will be read at most once during the bigrams computation. Thus, the cumlated time needed to compute bigrams of \emph{node.text} is linear.

\linearAlgo

% End of proof
To conclude the proof that algorithm \ref{linearAlgo} runs in linear time we show that all its inner loop computations other than bigrams of \emph{node.text} can be done in constant \emph{amortized} time. As the number of edges in a tree is one less than the number of nodes, the \emph{amortised} number of bigram unions per inner loop iteration is one. Each \emph{quotient of similarity} computation requires one bigram intersection and three bigram length computations. Over a finite alphabet (we used printable ASCII), bigram sizes have bounded size and each of these operations can be done in constant time.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Variations for authors, dates and comments}
% Intro
In the current state algorithms \ref{extractionAlgo} and \ref{linearAlgo} will performe poorly for author and date extraction and are not suitable for comment extraction. This subsection presents variations on the general extraction techniques presented before. Due to space constraints, we will only provide the main ideas and omit algorithmic and implementation details.

% Authors
The case of authors is problematic because it is not uncommon to encounter the author's name at several places in a page, leading to several rules with maximum score. The heuristic we use to get around this issue consists of adding a new component in the scores of authors extraction rules: the \emph{tree distance} between the evaluated node and the post content node. This new score component takes advantage of the positioning of post's authors which is often a direct child or shares parent with the post content node.

% Dates, authors
Dates are affected by the same duplication issue, as well as the inconsistence of format between web feeds and web pages. Our solution is to use multiple \emph{target} strings, while extending the rule that we previously described for author extraction. For instance, if the feed indicates that a post was published on
\stringliteral{Thu, 01 Jan 1970 00:00:00}, our algorithm will create a rule that extracts one of \stringliteral{Thursday January 1, 1970}, \stringliteral{1970-01-01}, \stringliteral{43 years ago} and so on. We use a list of \TODO{none so far :(} target date formats that was empirically built during our experiments. While this list is certainly not exhaustive, our approach using string similarity on multiple target dates is robust and tolerant to format variations. We do not, so far, support dates in multiple languages but English was sufficient for our experiments.

% Comments
Comments are usually available in separate web feeds, one per blog post. Similarly to blog feeds, comment feeds have a limited number of entries, and in case a blog post has more comments than the ones appearing in the feed, comments have to be extracted from the post web page. To do so we use algorithm \ref{extractionAlgo} with the following modifications:
\begin{enumerate}
  \item\label{lessthanfeedMod} Rules that return less HTML nodes than the number of comments on the feed are filtered out.
  \item\label{assignmentMod} The score of a rule is the value of the \emph{maximum weighted matching} in the \emph{complete bipartite graph} $G = (U, V, E)$, where $U$ is the set of HTML nodes returned by the rule, $V$ is the set of target comment fields from the web feed (such as comment authors) and $E(u, v)$ as weight equal to \code{\ref{similarityAlgo}(}$u, v$\code{)}.
\end{enumerate}
Our crawler executes this algorithm on each post that present a comment feed overflow, thus supporting blogs with multiple commenting engines. Comment contents are extracted first, which allows to narrow down modifications \ref{lessthanfeedMod} by fixing the target number of comments.
