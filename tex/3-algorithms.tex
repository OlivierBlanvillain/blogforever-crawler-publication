\section{Algorithms}

This section explains in details the algorithm we developed to extract blog posts content and it's variations for authors, date and comments. We will see how we can take advantage of blog specific characteristics to build extraction rules applicable to all posts of a blog. A careful attention will be paid to minimizing algorithmic complexity while keeping the approach simple and general.


%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Motivation}
% HTML extraction is non trival
Extracting metadata and content from HTML is not easy. Standards and recommendations have been present for quite some time, and tend to be quite strict on how the HTML documents should be organized. For instance the \code{<h1></h1>} tags should contain the highest-level heading of the page and must not be used more than once per page\cite{w3c2002}. More recently, specifications such as Microdata\cite{whatwg2013} define ways to nest semantics and metadatas inside HTML documents, but as of today suffer from very low usage rates, under 0.5\%\cite{andrewrogers2013}. In fact, a majority of web designer rely on the generic \code{<span></span>} and \code{<div></div>} containers with a custom \code{class} or \code{id} attributes to organize HTML pages. Under such consistences, relying on HTML structure to extract data from web pages it's not viable and other techniques have to be employed.

% Assumptions when working with blogs
\Anotecontent{noteA}{We observed during evaluation that all failing test blogs where violating one of these assumptions.}

Working with blogs allows to make assumptions\Anote{noteA} that will be central in our extraction procedure:
\begin{enumerate}[label={(\arabic*)}]
  \item \label{havefeed} Blogs provide web feeds that give a structured and standardized view of the blogs most recent posts,
  \item \label{similarhtml} Posts of a same blog share a similar HTML structure.
\end{enumerate}
On average, web feeds only contain about 20 entries\cite{oita2010}, less than the total number of posts in an average blog. In order to effectively archive old content from blogs it is necessary to download and process pages beyond the one referenced by the feed.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Content extraction}
% Per blog procedure, related work
In order to extract content, authors, date and comments of posts of a blog, we will proceed by first building extraction rules from the information given on the web feed. These rules will take advantage of the HTML structure similarities between pages of a same blog in order to be applicable to all posts of a blog. The approach is not new and has already showed to give great results (\cite{gkotsis2013}, \cite{oita2010}), outperforming a more generic state-of-the-art extraction algorithms which works directly from the HTML page, without the need for a web feed.

\begin{algorithm}
  \caption{Best Extraction Rule}\label{extraction}
  \setstretch{1.1}
  \thinspace
  \SetKw{Of}{of}
  \SetKw{In}{in}
  \SetKwFunction{Pair}{}
  \SetKwFunction{Apply}{Apply}
  \SetKwFunction{AllRules}{AllRules}
  \SetKwFunction{Similarity}{Similarity}
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \DontPrintSemicolon

  \Input{Set $pageZipTarget$ of Html and Text pairs}
  \Output{Best extraction rule}
  \BlankLine
  $topRules \longleftarrow new~list$\;
  \ForEach{\Pair{page, target} \In $pageZipTarget$}{
    $score \longleftarrow new~map$\;
    \ForEach{$rule$ \In \AllRules{page}}{
      $extracted \longleftarrow$ \Apply{rule, page}\;
      $score$ \Of $rule \longleftarrow$ \Similarity{extracted, target}\;
    }
    $topRules \longleftarrow topRules$ + rule with highest $score$\;
  }
  \Return{\emph{rule with highest occurrence in} $topRules$}\;
\end{algorithm}

% Algo, inputs/output
Algorithm \ref{extraction} shows the procedure we use to build the extraction rule for content of blog posts. It takes as input a set of pairs of HTML pages and target contents, and returns a extraction rule. The returned extraction rule should be such that when applied to one of the input page, the extracted text is close to the target content of the corresponding page.

% General comment, embarrassingly parallel
The idea is quite simple: compute the best extraction rule for each page/target tuple and return the most frequent rule. Making each of these computation independent might seems like a lose of information compared to another solution such as setting score of each rule to the average of it's score over all pages of the feed. However observed several cases were the independent solution performs better. When the web feed has a large portion of media only posts or with very short text, mean based solution tend to be more influenced by these outliers and are more likely to return wrong rules with constant low scores. In addition, having a independent computation for each input makes the algorithm \emph{embarrassingly parallel}: iterations of the outer loop are independent and can be executed on multiple threads.

% Input from assumptions.
Given a blog, the crawler can use assumption \ref{havefeed} to obtain inputs for Algorithm \ref{extraction}. Indeed a web feeds provide either a textual content or a textual description for each of it's entries, as well as a link to the corresponding page. Assumptions \ref{similarhtml} suggests the existence of solution: an extraction rule appropriate for each input pages/target content pairs. \ref{similarhtml} is also key to ensure applicability of the rule to all posts of the blog.

- XPath selectors (id, else class, else path)
- string similarity, caching



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Variations for comments, date and authors extraction}
- comments
- date authors
