\section{Algorithms}

This section explains in details the algorithm we developed to extract blog posts content and it's variations for authors, date and comments. We will see how we can take advantage of blog specific characteristics to build extraction rules applicable to all posts of a blog. A careful attention will be paid to minimizing algorithmic complexity while keeping the approach simple and general.


%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Motivation}
% HTML extraction is non trival
Extracting metadata and content from HTML is not easy. Standards and recommendations have been present for quite some time, and strict specify how HTML documents should be organized. For instance the \code{<h1></h1>} tags should contain the highest-level heading of the page and must not appear more than once per page\cite{w3c2002}. More recently, specifications such as Microdata\cite{whatwg2013} define ways to nest semantics and metadatas inside HTML documents, but as of today suffer from very low usage rates, under 0.5\%\cite{andrewrogers2013}. In fact, a majority of websites rely on the generic \code{<span></span>} and \code{<div></div>} containers with custom \code{class} or \code{id} attributes to organize HTML pages. Under such consistences, relying on HTML structure to extract data from web pages it's not viable and other techniques have to be employed.

% Assumptions when working with blogs
\Anotecontent{assumptions}{Our experiments on a large dataset of blogs showed than most of the failing blogs are violating one of these assumptions.}

With web blogs as our target websites we made the following assumptions\Anote{assumptions} that will be central in our extraction procedure:
n procedure:
\begin{enumerate}[label={(\arabic*)}]
  \item \label{havefeedAssum} Blogs provide web feeds that give a structured and standardized view of the blogs most recent posts,
  \item \label{similarhtmlAssum} Posts of a same blog share a similar HTML structure.
\end{enumerate}
On average, web feeds only contain about 20 entries\cite{oita2010}, less than the total number of posts in an average blog. Consequently, in order to effectively archive old content from blogs it is necessary to download and process pages beyond the one referenced by the feed.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Content extraction}
% Per blog procedure, related work
In order to extract content, authors, date and comments of posts of a blog, we will proceed by first building extraction rules from the information given on it's web feed. These rules will take advantage of the similarities between HTML structure of pages on a same blog in order to be applicable to all posts of a blog. The approach is not new and has already showed to give great results (\cite{gkotsis2013}, \cite{oita2010}), outperforming more generic extraction algorithms that works directly on HTML pages, without the need for a web feed.

\extractionAlgo

% Algorithm, inputs/output
Algorithm \ref{extractionAlgo} shows the procedure we use to build the extraction rule for content of blog posts. It takes as input a set of pairs of HTML pages and target contents, and returns an extraction rule. This rule should be such that when applied to one of the input pages, the extracted text is close to the corresponding target content.

% General comment, embarrassingly parallel
The idea is quite simple: compute the best extraction rule for each page/target pair and return the most frequent best rule. Making each of these computation independent might seems like a lose of information compared to alternative solutions such as having a global score for each rule computed as the average of it's score over all pages of the feed. We observed in our tests several cases were the independent solution performs better. When the web feed has a large portion of posts with very few text, mean based solutions tend to be more influenced by these outliers and are inclined to return wrong results with constant low scores. In addition, having a independent computation for each input makes the algorithm \emph{embarrassingly parallel}: iterations of the outer loop can trivialy be executed by multiple threads.

% Input from assumptions.
Given a blog, the crawler can use assumption \ref{havefeedAssum} to obtain inputs for Algorithm \ref{extractionAlgo}. Indeed a web feeds provide either a textual content or a textual description for each of it's entries, as well as a link to the corresponding page. Assumptions \ref{similarhtmlAssum} suggests the existence of solution: an extraction rule appropriate for each input pages/target content pairs. \ref{similarhtmlAssum} is also key to ensure applicability of the rule to all posts of the blog.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Extraction rules and string similarity}
% XPath selectors (id, else class, else path)
In our implementation, rules are queries in the XML Path Language (XPath). Consequently, standard libraries can be used parse html pages and apply extraction rules, thus providing the \code{apply} function use Algorithm \ref{extractionAlgo}. We experimented with 3 types of XPath queries: selection over \code{id} attribute, selection over \code{class} attribute and selection with absolute path. We have found that ignoring nodes without \code{id} or \code{class} absolutes prunes out a lot of uninteresting HTML nodes without decreasing  the quality of the result. Also when both \code{id} and \code{class} attributes are present the \code{id} attribute shows to be a stronger selector, here matching the HTML specification regarding the uniqueness of \code{id}. This observations led to the following \code{\ref{allrulesAlgo}} function:

\allrulesAlgo

% String similarity
Unsurprisingly, the choice of string similarity has a strong influence on the running time and precision of the extraction procedure. We use Sørensen–Dice coefficient similarity\cite{dice1945}, which is to the best of our knowledge the only string similarity fulfilling the following criteria:

\begin{enumerate}
  \item \label{wordorderProp} Have low sensibility to word ordering
  \item \label{substringProp} Favor substrings over strings of similar length
  \item \label{lineraProp} Run in linear time
\end{enumerate}

Properties \ref{wordorderProp} and \ref{substringProp} are essential to deal with cases where web feeds contain only an abstract or the first paragraph of the post content. The following table illustrates these two properties with examples, also giving the Levenshtein\cite{levenshtein1966} similarity to illustrate the poor performance of string matching based similarities.

\similarityTable

The Sørensen–Dice coefficient methods operates by first building sets of pairs of adjacent characters, also knows as \emph{Bigrams}, and then applying the \emph{quotient of similarity} formula:

\similarityAlgo

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Linear time reformulation}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Variations for comments, date and authors extraction}
- comments
- date authors
