\section{Algorithms}

This section explains in details the algorithm we developed to extract blog posts content and it's variations for authors, date and comments. We will see how we can take advantage of blog specific characteristics to build extraction rules applicable to all posts of a blog. A careful attention will be paid to minimizing algorithmic complexity while keeping the approach simple and general.


%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Motivation}
% HTML extraction is non trival
Extracting metadata and content from HTML is not easy. Standards and recommendations have been present for quite some time, and tend to be quite strict on how the HTML documents should be organized. For instance the \code{<h1></h1>} tags should contain the highest-level heading of the page and must not be used more than once per page\cite{w3c2002}. More recently, specifications such as Microdata\cite{whatwg2013} define ways to nest semantics and metadatas inside HTML documents, but as of today suffer from very low usage rates, under 0.5\%\cite{andrewrogers2013}. In fact, a majority of web designer rely on the generic \code{<span></span>} and \code{<div></div>} containers with a custom \code{class} or \code{id} attributes to organize HTML pages. Under such consistences, relying on HTML structure to extract data from web pages it's not viable and other techniques have to be employed.

% Assumptions when working with blogs
\Anotecontent{noteA}{We observed during evaluation that all failing test blogs where violating one of these assumptions.}

Working with blogs allows to make assumptions\Anote{noteA} that will be central in our extraction procedure:
\begin{enumerate}[label={(\arabic*)}]
  \item \label{havefeed} Blogs provide web feeds that give a structured and standardized view of the blogs most recent posts,
  \item \label{similarhtml} Posts of a same blog share a similar HTML structure.
\end{enumerate}
On average, web feeds only contain about 20 entries\cite{oita2010}, less than the total number of posts in an average blog. In order to effectively archive old content from blogs it is necessary to download and process pages beyond the one referenced by the feed.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Content extraction}
% \Anotecontent{vague}{The definition of the objective extraction rule is voluntarily vague, }

% Per blog procedure, inputs/output
Algorithm \ref{extraction} we are going to present in this section as for function to build an extraction rule for a relevant data of blog posts. It takes an input a set of pairs of HTML pages and target contents, and returns a extraction rule. The returned extraction rule should be such that when applied to one of the input page, the extracted text is close to the target content of the corresponding page.


\begin{algorithm}
  \caption{Best Extraction Rule}\label{extraction}
  \setstretch{1.1}
  \thinspace
  \SetKw{Of}{of}
  \SetKw{In}{in}
  \SetKwFunction{Pair}{}
  \SetKwFunction{Apply}{Apply}
  \SetKwFunction{AllRules}{AllRules}
  \SetKwFunction{Similarity}{Similarity}
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \DontPrintSemicolon

  \Input{Set $pageZipTarget$ of Html and Text pairs}
  \Output{Best extraction rule}
  \BlankLine
  $topRules \longleftarrow new~list$\;
  \ForEach{\Pair{page, target} \In $pageZipTarget$}{
    $score \longleftarrow new~map$\;
    \ForEach{$rule$ \In \AllRules{page}}{
      $extracted \longleftarrow$ \Apply{rule, page}\;
      $score$ \Of $rule \longleftarrow$ \Similarity{extracted, target}\;
    }
    $topRules \longleftarrow topRules$ + rule with highest $score$\;
  }
  \Return{\emph{rule with highest occurrence in} $topRules$}\;
\end{algorithm}


Given a blog, the crawler can use assumption \ref{havefeed} to obtain inputs for ALGO1. Indeed a web feeds provide either a textual content or a textual description for each of it's entries, as well as a link to the corresponding page. Assumptions \ref{similarhtml} suggests the existence of solution: an extraction rule appropriate for each input pages/target content pairs. \ref{similarhtml} is also key to ensure applicability of the rule to all posts of the blog.

- related algorithms, general brute force viewpoint
- algorithm
- XPath selectors (id, else class, else path)
- string similarity
- caching


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Variations for comments, date and authors extraction}
- comments
- date authors
