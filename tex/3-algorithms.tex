\section{Algorithms}\label{algorithms}

This section explains in detail the algorithms we developed to extract blog posts' content and its variations for authors, dates and comments.
We explain how we take advantage of blog specific characteristics to build extraction rules applicable to all posts of a blog.
Our focus is on minimising the algorithmic complexity while keeping our approach simple and generic.

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Motivation}
% html extraction is not trival
Extracting metadata and content from HTML documents is a challenging task. Standards and format recommendations have been around for quite some time, strictly specifying how HTML documents should be organised \cite{w3c2013}. For instance the \code{<h1></h1>} tags should contain the highest-level heading of the page and must not appear more than once per page\cite{w3c2002}. More recently, specifications such as Microdata\cite{whatwg2013} define ways to embed semantic information and metadata inside HTML documents, but these still suffer from very low usage: estimated to be used in less than 0.5\%\cite{andrewrogers2013} of websites. In fact, the majority of websites rely on the generic \code{<span></span>} and \code{<div></div>} container elements with custom \code{id} or \code{class} attributes to organise structure pages\cite{brianwilson2008}, and more than 95\% of pages do not pass HTML validation\cite{brianwilson2008-a}. Under such circumstances, relying on HTML structure to extract content from web pages is not viable and other techniques need to be employed.

\TODO{Assumption is not a good word, we are based on facts from previous work.}
% I suggest that you restructure this part as follows:

% Having blogs as our target websites, we are taking into account  the following facts that play a central role in the extraction process.

% 1. "Blog is a kind of web page [with] frequent, usually brief posts, with the immediacy of reverse chronological order" (+ ref to BlogForever D2.2: BlogForever Data Model
% https://zenodo.org/record/7488/files/BlogForever\_D2\_2WeblogDataModel.pdf

% 2. blogs provide web feeds: structured and standardised views of the most recent blog posts.

% 3. Web feeds contain information regarding the latest blog posts (usually 10 or 20 entries) + ref

% 4. posts of the same blog share a similar HTML structure.

% Consequently, in order to effectively archive old blog content, it is necessary to download and process pages beyond the ones referenced in web feeds.}

\Anotecontent{assumptions}{Our experiments on a large dataset of blogs showed that failing tests were either due to a violation of one of these assumptions, or to an insufficient amount of text in posts, such as \emph{photoblogs} with short captions.}

% assumptions when working with blogs
Having blogs as our target websites, the assumptions we made and which play a central role in the extraction process are the following\Anote{assumptions}:
\begin{enumerate}[label={(\arabic*)}]
  \item\label{havefeedAssum} Blogs provide web feeds: structured and standardized views of the most recent posts of a blog,\TODO{mention RSS and Atom, just to confirm to the reader what a web feed is?}
  \item\label{similarhtmlAssum} Posts of the same blog share a similar HTML structure.
\end{enumerate}
Web feeds usually contain about 20 posts\cite{oita2010},\TODO{Add more info on web feeds, merge with the text on intro.} often less than the total number of posts in blogs. Consequently, in order to effectively archive old content from blogs, it is necessary to download and process pages beyond the ones referenced in the feed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Content extraction}
\TODO{When I read this section for the first time, I was confused.
After I also read 3.3 and 3.4 I understood better.
You should have a nicely structured overview of the algorithm in 3.2 and have references for 3.3 and 3.4
When you describe the algorithm overview, you should say explicitly that function X and operation Y is explained in section Z, etc}
% per blog procedure, related work
To extract content, authors, dates and comments from blog posts, we started by building extraction rules from the data given in the blog's web feed. \TODO{These rules take advantaged of the facts presented in 3.1 to create a more efficient data extraction process, outperforming more generic extraction algorithms that work directly with HTML pages.}These rules take advantage of our assumption that the HTML structure between different posts of the same blog is the same, in order to be applicable to all the posts of a blog. This approach has been examined in the past and has demonstrated great results (see \cite{gkotsis2013}, \cite{oita2010}), outperforming more generic extraction algorithms that work directly on HTML pages.\comment{This might be a bit too raw, give more context at this point?}

\extractionAlgo

% algorithm, inputs/output
Algorithm \ref{extractionAlgo} shows the steps we follow to build extraction rules for the content of blog posts. As input, it takes a set of pairs of HTML pages and target contents, and finally returns an extraction rule. This rule is such that when applied to other input pages, the extracted contents are as similar as possible to the corresponding target contents.\TODO{You definitely need a better explanation}

% general idea, embarrassingly parallel
\TODO{This paragraph is not clear.}
The idea is quite simple: to compute the best extraction rule for each pair of HTML pages and target content and to finally return the most frequent best rule. Making each of these computations independent might seem like a loss of information compared to alternative solutions such as having a global score for each rule, computed as the average of each rule's score over all the pages of the feed. In our experimentation we observed several cases where the independent solution performs better\TODO{example, reference or proof?}. When web feeds have posts with very short content, mean based solutions tend to be more influenced by these outliers and are inclined to return erroneous rules which fail to return target contents but constantly have an average score.\TODO{rephrase!}

In addition, having an independent computation for each input makes the algorithm \emph{embarrassingly parallel}: iterations of the outer loop can trivially be executed by multiple threads.

% input from assumptions.
Before going into further details about annex functions and time complexity, we should explain how the previously stated assumptions relate to Algorithm \ref{extractionAlgo}. Given a blog, assumption \ref{havefeedAssum} allows the crawler to obtain input for the algorithm. Indeed, web feeds provide either textual content or descriptions for their entries, as well as links to the corresponding pages. Assumption \ref{similarhtmlAssum} guarantees the existence of an appropriate extraction rule, as well as its applicability to all the posts of a blog.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Extraction rules and string similarity}
% xpath selectors (id, else class, else path)
In our implementation, rules are queries in the XML Path Language (XPath). Consequently, standard libraries can be used to parse HTML pages and apply extraction rules, thus providing the \code{Apply} function, used in Algorithm \ref{extractionAlgo}. We experimented with 3 types of XPath queries: selection over the HTML \code{id} attribute, selection over the HTML \code{class} attribute and selection with the absolute path in the HTML tree. \code{id} attributes are expected to be unique, and \code{class} attributes have showed to have better blog consistency than absolute paths. For these reasons, the \coderef{allrulesAlgo} function only returns the best rule for each node:

\allrulesAlgo

% string similarity
Unsurprisingly, the choice of the string similarity algorithm greatly influences the running time and precision of the extraction process. We chose the Sørensen–Dice coefficient similarity\cite{dice1945}, which, to the best of our knowledge, is the only string similarity algorithm fulfilling the following criteria:

\begin{enumerate}
  \item\label{wordorderProp} Has low sensitivity to word ordering
  \item\label{lengthProp} Has low sensitivity to length variations
  \item\label{linearProp} Runs in linear time
\end{enumerate}

Properties \ref{wordorderProp} and \ref{lengthProp} are essential to deal with cases where a blog's web feed only contains an abstract or a subset of the entire post content. Table \ref{similarityTable} gives examples to illustrate how these two properties are fulfilled by the Sørensen–Dice coefficient similarity but do not hold for \emph{edit distance} based similarities such as the Levenshtein\cite{levenshtein1966} similarity.

\similarityTable

The Sørensen–Dice coefficient method operates by first building sets of pairs of adjacent characters, also knows as \emph{bigrams}, and then applying the \emph{quotient of similarity} formula:

\TODO{similatiry -> score function}
\similarityAlgo

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Time complexity and linear reformulation}
% algo 1 is quadratic
With both the \coderef{allrulesAlgo} and \coderef{similarityAlgo} functions defined, we will show that Algorithm \ref{extractionAlgo} has a quadratic running time. First, let's assume that we have at our disposal a linear time HTML parser that constructs an appropriate data structure indexing HTML nodes on their \code{id} and \code{class} attributes, effectively making \code{Apply}~$\in$~\Oof{1}. As stated before the outer loop splits the input into independent computations and each call to \coderef{allrulesAlgo} returns (in linear time) at most as many rules as the number of nodes in its \emph{page} argument. Therefore, the body of the inner loop will be executed \Oof{n} times. Because the extraction rule can return any subtree of the queried page, each call to \coderef{similarityAlgo} takes \Oof{n}, leading to an overall quadratic running time.

% Linear reformulation
We will now present Algorithm \ref{linearAlgo}, a dynamic programming reformulation of Algorithm \ref{extractionAlgo} with linear running time. While very intuitive, the original idea of first generating extraction rules and then picking these best rules prevents us from an effective reuse of previously computed bigrams. For instance, when evaluating the extraction rule for the HTML root node, Algorithm \ref{extractionAlgo} will obtain the complete string of the page and pass it to the \coderef{similarityAlgo} function. At this point, the information on where the string could be split into substrings with already computed bigrams is not accessible, and the bigrams of the page have to be computed by linearly traversing the entire string. To overcome this limitation and implement \emph{memoization} over the bigram computations, Algorithm \ref{linearAlgo} uses a post-order traversal of the HTML tree to compute node bigrams from their children bigrams. This way we avoid serializing HTML subtrees for each bigram computation and we can guarantee that each character of the HTML page will be read at most once during the bigrams computation. Thus, the cumulated time needed to compute bigrams of \emph{node.text} is linear.

\linearAlgo

% End of proof
To conclude the proof that algorithm \ref{linearAlgo} runs in linear time we show that all its inner loop computations other than bigrams of \emph{node.text} can be done in constant \emph{amortized} time. As the number of edges in a tree is one less than the number of nodes, the \emph{amortised} number of bigram unions per inner loop iteration is one. Each \emph{quotient of similarity} computation requires one bigram intersection and three bigram length computations. Over a finite alphabet (we used printable ASCII), bigram sizes have bounded size and each of these operations can be done in constant time.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Variations for authors, dates and comments}
% Intro
In the current state algorithms \ref{extractionAlgo} and \ref{linearAlgo} will perform poorly for author and date extraction and are not suitable for comment extraction. This subsection presents variations on the general extraction techniques presented before. \TODO{Rephrase this to say that we don't show the full algos because it's just as in the 2 previous with customised score function!}Due to space constraints, we will only provide the main ideas and omit algorithmic and implementation details.

% Authors
The case of authors is problematic because it is not uncommon to encounter the author's name at several places in a page, leading to several rules with maximum score. The heuristic we use to get around this issue consists of adding a new component in the scores of authors extraction rules: the \emph{tree distance} between the evaluated node and the post content node. This new score component takes advantage of the positioning of post's authors which often is a direct child or shares its parent with the post content node.

% Dates, authors
Dates are affected by the same duplication issue, as well as the inconsistence of format between web feeds and web pages. Our solution is to use multiple \emph{target} strings, while extending the rule that we previously described for author extraction. For instance, if the feed indicates that a post was published on
\stringliteral{Thu, 01 Jan 1970 00:00:00}, our algorithm will create a rule that extracts one of \stringliteral{Thursday January 1, 1970}, \stringliteral{1970-01-01}, \stringliteral{43 years ago} and so on. We use a total of 8 target date formats that was empirically built during our experiments. While this list is certainly not exhaustive, our approach using string similarity on multiple target dates is tolerant to small variations. So far we do not support dates in multiple languages, but adding new target formats base and languages detection would certainly by a possible extension of our date extraction algorithm.

% Comments
Comments are usually available in separate web feeds, one per blog post. Similarly to blog feeds, comment feeds have a limited number of entries, and in case a blog post has more comments than the ones appearing in the feed, comments have to be extracted from the post web page. To do so we use algorithm \ref{extractionAlgo} with the following modifications:
\begin{enumerate}
  \item Rules that return less HTML nodes than the number of comments on the feed are filtered out.
  \item The score of a rule is the value of the \emph{maximum weighted matching} in the \emph{complete bipartite graph} $G = (U, V, E)$, where $U$ is the set of HTML nodes returned by the rule, $V$ is the set of target comment fields from the web feed (such as comment authors) and $E(u, v)$ as weight equal to \code{\ref{similarityAlgo}(}$u, v$\code{)}.
\end{enumerate}
Our crawler executes this algorithm on each post that present a comment feed overflow, thus supporting blogs with multiple commenting engines. Comment contents are extracted first, which allows to narrow down the first modification by fixing a target number of comments.
