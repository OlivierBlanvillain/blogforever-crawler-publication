\section{Algorithms}

This section explains in details the algorithm we developed to extract blog posts content and it's variations for authors, date and comments. We will see how we can take advantage of blog specific characteristics to build extraction rules applicable to all posts of a blog. A careful attention will be paid to minimizing algorithmic complexity while keeping the approach simple and general.


%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Motivation}
% html extraction is non trival
Extracting metadata and content from HTML is not easy. Standards and recommendations have been present for quite some time, strictly specifying how HTML documents should be organized. For instance the \code{<h1></h1>} tags should contain the highest-level heading of the page and must not appear more than once per page\cite{w3c2002}. More recently, specifications such as Microdata\cite{whatwg2013} define ways to nest semantics and metadatas inside HTML documents, but as of today suffer from very low usage rates, under 0.5\%\cite{andrewrogers2013}. In fact, a majority of websites rely on the generic \code{<span></span>} and \code{<div></div>} containers with custom \code{id} or \code{class} attributes to organize pages, and it is not uncommon to enconter incorrect HTML. Under such consistences, relying on HTML structure to extract data from web pages it's not viable and other techniques have to be employed.

\Anotecontent{assumptions}{Our experiments on a large dataset of blogs showed than failing tests where either due to a violation of one of these assumptions, or to an insufficient amount of text in posts, such as \emph{photoblogs} with short captions.}

% assumptions when working with blogs
With web blogs as our target websites we made the following assumptions that will play a central role in the extraction procedure\Anote{assumptions}:
\begin{enumerate}[label={(\arabic*)}]
  \item\label{havefeedAssum} Blogs provide web feeds: a structured and standardized view of blogs most recent posts,
  \item\label{similarhtmlAssum} Posts of a same blog share a similar HTML structure.
\end{enumerate}
Web feeds contain about 20 entries\cite{oita2010}, often less than the total number of posts in blogs. Consequently, in order to effectively archive old content from blogs it is necessary to download and process pages beyond the one referenced by the feed.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Content extraction}
% per blog procedure, related work
To extract content, authors, date and comments of blog posts, we will proceed by first building extraction rules from the information given on it's web feed. These rules will take advantage of the similarities between HTML structure of pages on a same blog in order to be applicable to all posts of a blog. The approach is not new and has already showed to give great results (see \cite{gkotsis2013}, \cite{oita2010}), outperforming more generic extraction algorithms that works directly on HTML pages.

\extractionAlgo

% algorithm, inputs/output
Algorithm \ref{extractionAlgo} shows the procedure we use to build the extraction rule for blog post contents. It takes as input a set of pairs of HTML pages and target contents, and returns an extraction rule. This rule will be such that when applied input pages, the extracted texts are close to the corresponding target contents. \comment{this last sentence might not be very clear (?), but this is very important.}%

% general idea, embarrassingly parallel
The idea is quite simple: compute the best extraction rule for each page/target pair and return the most frequent best rule. Making each of these computation independent might seems like a lose of information compared to alternative solutions such as having a global score for each rule, computed as the average of it's score over all pages of the feed. We observed in our experimentation several cases were the independent solution performs better. When web feeds have many posts with very few text, mean based solutions tend to be more influenced by these outliers and are inclined to return wrong rules which never return target contents but constantly have an average score.

In addition, having a independent computation for each input makes the algorithm \emph{embarrassingly parallel}: iterations of the outer loop can trivialy be executed by multiple threads.

% input from assumptions.
Before going in details about annex functions and time complexity, we explain how the previously stated assumptions relate to Algorithm \ref{extractionAlgo}. Given a blog, assumption \ref{havefeedAssum} allows the crawler to obtain inputs for the algorithm. Indeed, web feeds provide either textual content or description of its entries, as well as a link to the corresponding page. Assumption \ref{similarhtmlAssum} garenties the existence of an appropriate an extraction rule, as well it's applicability to all posts of the blog.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Extraction rules and string similarity}
% xpath selectors (id, else class, else path)
In our implementation, rules are queries in the XML Path Language (XPath). Consequently, standard libraries can be used parse html pages and apply extraction rules, thus providing the \code{Apply} function used Algorithm \ref{extractionAlgo}. We experimented with 3 types of XPath queries: selection over the HTML \code{id} attribute, selection over the HTML \code{class} attribute and selection with path in the HTML tree. We have found that ignoring nodes without \code{id} or \code{class} attributes prunes out many uninteresting HTML nodes without decreasing  the quality of the result. Also when both \code{id} and \code{class} attributes are present, we can ignore the \code{class} has \code{id}s should be unique. This observations led to the following \coderef{allrulesAlgo} function:

\allrulesAlgo

% string similarity
Unsurprisingly, the choice of string similarity has a strong influence on the running time and precision of the extraction procedure. We use Sørensen–Dice coefficient similarity\cite{dice1945}, which is to the best of our knowledge the only string similarity fulfilling the following criterias:

\begin{enumerate}
  \item\label{wordorderProp} Have low sensibility to word ordering
  \item\label{substringProp} Favor substrings over strings of similar length
  \item\label{linearProp} Run in linear time
\end{enumerate}

Properties \ref{wordorderProp} and \ref{substringProp} are essential to deal with cases where web feeds contain only an abstract or a subset of the post content. Table \ref{similarityTable} illustrates these two properties with examples, also giving the Levenshtein\cite{levenshtein1966} similarity to illustrate poor performances of string matching based similarities.

\similarityTable

The Sørensen–Dice coefficient methods operates by first building sets of pairs of adjacent characters, also knows as \emph{bigram}, and then applying the \emph{quotient of similarity} formula:

\similarityAlgo


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Time complexity and linear reformulation}
% algo 1 is quadratic
With both \coderef{allrulesAlgo} and \coderef{similarityAlgo} function defined, we will now show that Algorithm \ref{extractionAlgo} has a quadratic running time. Let's first assume that we have at our disposal a linear time HTML parser and a data-structure indexing HTML nodes on their \code{id} and \code{class} attributes, effectively making \code{Apply}~$\in$~\Oof{1}. As stated before, the outer loop splits the input into independent computations, and each call to \coderef{allrulesAlgo} returns (in linear time) less rules than the number of nodes of the HTML tree argument. Therefore, the body of the inner loop will be executed a linear number of time. Because extraction rule can return arbitrary subtrees of the queried page, each call to \coderef{similarityAlgo} takes \Oof{n}, leading to an overall quadratic running time.

\linearAlgo

% linear reformulation
We will now present Algorithm \ref{linearAlgo}, a dynamic programming reformulation of Algorithm \ref{extractionAlgo} with linear running time. While very intuitive, the original idea of first generating extraction rules and then picking these best rule prevents an effective reuse of previously computed bigrams. For instance, when evaluating the extraction rule for the HTML root node, Algorithm \ref{extractionAlgo} will obtain the complete string of the page and pass it to the \coderef{similarityAlgo} function. At this point, the information of where the string could be spitted into substring with already computed bigrams is not accessible, and the bigrams of the page has to be computed by linearly traversing the full string. To overcome this limitation and implement an effective $memoization$ \comment{without R!}over the bigrams computations, Algorithm \ref{linearAlgo} uses a post-order traversal of the HTML tree to compute node bigrams from it's children bigrams, effectively avoiding serialization of HTML subtrees.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Variations for authors, dates and comments}
% Intro
As is, algorithms \ref{extractionAlgo} and \ref{linearAlgo} will performed poorly for authors and dates extraction and are not suitable for comment extraction. This subsection presents variations of the general extraction techniques presented before. Due to space constraints, we will only provide the main ideas and omit algorithmic and implementation details.

% Authors
The case of authors is problematic because it is not uncommon to encounter the author's name at several places in a page, leading to several rules with maximum score. The heuristic we use to get around this issue consists in adding a new component in the scores of authors extraction rules: the \emph{tree distance} between the evaluated node and the post content node. This new score component takes advantage of the positioning of post's authors which is often a direct child or shares parent with the post content node.

% Dates, authors
Dates are affected by the same duplication issue, as well as inconsistence of format between web feeds and web pages. Our solution to build date extraction rules extends what we presented before for authors by using multiple \emph{target} strings. For instance, if the feed indicates that a post was published on \stringliteral{2013-12-09}, our algorithm will search for a rule to extract one of \stringliteral{}, \stringliteral{}, \stringliteral{} and so on. We use a list of \TODO{0} target date formats that was empirically built during our experiments. While this list is certainly not exhaustive, our approach using string similarity on multiple target is robust to format changes. We did not took the time to support dates in multiple languages but it would be a straightforward modification.

% Comments
Comments are usually available is separated web feeds, organized by blog post. Similarity to blog feeds, comment feeds have a limited number en entries, and in case of overflow content has to be extracted from the post web page.

