\section{Algorithms}
\label{algorithms}

This section explains in detail the algorithms we developed to extract blog posts' article and its variations for authors, dates and comments.
We explain how we take advantage of blog specific characteristics to build extraction rules applicable to all posts of a blog.
Our focus is on minimising the algorithmic complexity while keeping our approach simple and generic.


%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Motivation}
\label{motivation}

% html extraction is not trival
extracting metadata and content from html documents is a challenging task. standards and format recommendations have been around for quite some time, strictly specifying how html documents should be organised \cite{w3c2013}. for instance the \code{<h1></h1>} tags should contain the highest-level heading of the page and must not appear more than once per page\cite{w3c2002}. more recently, specifications such as microdata\cite{whatwg2013} define ways to embed semantic information and metadata inside html documents, but these still suffer from very low usage: estimated to be used in less than 0.5\%\cite{andrewrogers2013} of websites. in fact, the majority of websites rely on the generic \code{<span></span>} and \code{<div></div>} container elements with custom \code{id} or \code{class} attributes to organise structure pages\cite{brianwilson2008}, and more than 95\% of pages do not pass html validation\cite{brianwilson2008-a}. under such circumstances, relying on html structure to extract content from web pages is not viable and other techniques need to be employed.

\Anotecontent{assumptions}{Our experiments on a large dataset of blogs showed that failing tests were either due to a violation of one of these assumptions, or to an insufficient amount of text in posts, such as \emph{photoblogs} with short captions.}

% assumptions when working with blogs
Having blogs as our target websites, we use the following observations \TODO{If ok, change everywhere else.} which play a central role in the extraction process\Anote{assumptions}:
\begin{enumerate}[label={(\arabic*)}]
  \item\label{havefeedAssum} Blogs provide web feeds: structured and standardized views of the latest posts of a blog
  \item\label{similarhtmlAssum} Posts of the same blog share a similar HTML structure.
\end{enumerate}
Web feeds usually contain about 20 posts\cite{oita2010}, often less than the total number of posts in blogs. \TODO{Add more info on web feeds, merge with the text on intro?} Consequently, in order to effectively archive old content from blogs, it is necessary to download and process pages beyond the ones referenced in the feed.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Content extraction}
\label{contentextraction}

% per blog procedure, related work
To extract articles, authors, dates and comments from blog posts, we procede by building extraction rules from the data given in the blog's web feed. The idea is to provide a set of \emph{training data} to the algorithm, pairs of HTML page and target content, which are then used to build an extraction rule capable of locating the target content on each HTML page.

% input from assumptions.
Assumption \ref{havefeedAssum} allows the crawler to obtain input for the algorithm: each entry of the web feed provides a link to the corresponding we page as well as potential target content such as the article, it's title, authors and publication date. Assumption \ref{similarhtmlAssum} guarantees the existence of an appropriate extraction rule, as well as its applicability to all the posts of a blog.

% textual pseudocode
Algorithm \ref{extractionAlgo} show the generic procedure we use to build extraction rules. The idea is quite simple: for each \code{(}\emph{page, target}\code{)} input, compute out of all extraction rules the best one with respect to a certain \code{ScoreFunction}. The rule which was the most frequently a \emph{best rule} is then returned.

\extractionAlgo

% embarrassingly parallel
One might notice that each \emph{best rule} computation is independent and operates on a different input pairs. Therefore algorithm \ref{extractionAlgo} \emph{embarrassingly parallel}: iterations of the outer loop can trivially be executed by multiple threads.

% abs, plan
Functions in algorithm \ref{extractionAlgo} are voluntarily abstract at this point and will be explained in details in the remaining of this section. \ref{extractionrulesandstringsimilarity} defines \code{AllRules}, \code{Apply} and the \code{ScoreFunction} we use for article and title extraction. In \ref{timecomplexityandlinearreformulation} we analyses time complexity of Algorithm \ref{extractionAlgo} and give a linear time reformulation using dynamic programing. Finally, \ref{variationsforauthorsdatesandcomments} shows how the \code{ScoreFunction} can be adapted to extract authors, dates and comments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Extraction rules and string similarity}
\label{extractionrulesandstringsimilarity}

% xpath selectors (id, else class, else path)
In our implementation, rules are queries in the XML Path Language (XPath). Consequently, standard libraries can be used to parse HTML pages and apply extraction rules, thus providing the \code{Apply} function, used in Algorithm \ref{extractionAlgo}. We experimented with 3 types of XPath queries: selection over the HTML \code{id} attribute, selection over the HTML \code{class} attribute and selection with the absolute path in the HTML tree. \code{id} attributes are expected to be unique, and \code{class} attributes have showed to have better blog consistency than absolute paths. For these reasons, the \coderef{allrulesAlgo} function only returns the best rule for each node:

\allrulesAlgo

% score function := string similarity
Unsurprisingly, the choice of \code{ScoreFunction} greatly influences the running time and precision of the extraction process. When targeting articles, extraction rules are scored with a string similarity function comparing extracted and target strings. We chose the Sørensen–Dice coefficient similarity\cite{dice1945}, which, to the best of our knowledge, is the only string similarity algorithm fulfilling the following criteria:

\begin{enumerate}
  \item\label{wordorderProp} Has low sensitivity to word ordering
  \item\label{lengthProp} Has low sensitivity to length variations
  \item\label{linearProp} Runs in linear time
\end{enumerate}

Properties \ref{wordorderProp} and \ref{lengthProp} are essential to deal with cases where a blog's web feed only contains an abstract or a subset of the entire post article. Table \ref{similarityTable} gives examples to illustrate how these two properties are fulfilled by the Sørensen–Dice coefficient similarity but do not hold for \emph{edit distance} based similarities such as the Levenshtein\cite{levenshtein1966} similarity.

\similarityTable

The Sørensen–Dice coefficient method operates by first building sets of pairs of adjacent characters, also knows as \emph{bigrams}, and then applying the \emph{quotient of similarity} formula:

\similarityAlgo


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Time complexity and linear reformulation}
\label{timecomplexityandlinearreformulation}

% into: algo 1 defined
With \coderef{allrulesAlgo}, \code{Apply} and \coderef{similarityAlgo} (as \code{ScoreFunction}) functions defined, the definition of algorithm \ref{extractionAlgo} for article extraction is completed. We now proceed with a time complexity analysis which will show that this aproach has a quadratic running time.

% algo 1 is quadratic
First, let's assume that we have at our disposal a linear time HTML parser that constructs an appropriate data structure indexing HTML nodes on their \code{id} and \code{class} attributes, effectively making \code{Apply}~$\in$~\Oof{1}. As stated before the outer loop splits the input into independent computations and each call to \coderef{allrulesAlgo} returns (in linear time) at most as many rules as the number of nodes in its \emph{page} argument. Therefore, the body of the inner loop will be executed \Oof{n} times. Because the extraction rule can return any subtree of the queried page, each call to \coderef{similarityAlgo} takes \Oof{n}, leading to an overall quadratic running time.

% linear reformulation
We now present Algorithm \ref{linearAlgo}, linear time reformulation of Algorithm \ref{extractionAlgo} for article extraction using dynamic programming. While very intuitive, the original idea of first generating extraction rules and then picking these best rules prevents us from an effective reuse of previously computed bigrams. For instance, when evaluating the extraction rule for the HTML root node, Algorithm \ref{extractionAlgo} will obtain the complete string of the page and pass it to the \coderef{similarityAlgo} function. At this point, the information on where the string could be split into substrings with already computed bigrams is not accessible, and the bigrams of the page have to be computed by linearly traversing the entire string. To overcome this limitation and implement \emph{memoization} over the bigram computations, Algorithm \ref{linearAlgo} uses a post-order traversal of the HTML tree to compute node bigrams from their children bigrams. This way we avoid serializing HTML subtrees for each bigram computation and have the guarantee that each character of the HTML page will be read at most once during the bigrams computation.

\linearAlgo

% proof
With bigrams computed in a dynamic programming manner, the cumulated time needed to compute \coderef{bigramsAlgo}\code{(}\emph{node.text}\code{)} is linear. To conclude the proof that algorithm \ref{linearAlgo} runs in linear time we show that all other computations of the inner loop can be done in constant \emph{amortized} time. As the number of edges in a tree is one less than the number of nodes, the \emph{amortised} number of bigram unions per inner loop iteration tends to one. Each \emph{quotient of similarity} computation requires one bigram intersection and three bigram length computations. Over a finite alphabet (we used printable ASCII), bigram sizes have bounded size and each of these operations takes constant time.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Variations for authors, dates and comments}
\label{variationsforauthorsdatesandcomments}

% intro
In the current state algorithms \ref{extractionAlgo} and \ref{linearAlgo} will perform poorly for author and date extraction and are not suitable for comment extraction. This subsection presents variations on the general extraction
\label{presentsvariationsonthegeneralextraction}techniques presented before. \TODO{Rephrase this to say that we don't show the full algos because it's just as in the 2 previous with customised score function!}Due to space constraints, we will only provide the main ideas and omit algorithmic and implementation details.

% authors
The case of authors is problematic because it is not uncommon to encounter the author's name at several places in a page, which results in several rules with maximum \coderef{similarityAlgo} score. The heuristic we use to get around this issue consists of adding a new component in the \code{ScoreFunction} for authors extraction rules: the \emph{tree distance} between the evaluated node and the post content node. This new component takes advantage of the positioning of post's authors which often is a direct child or shares its parent with the post content node.

% dates
Dates are affected by the same duplication issue, as well as the inconsistence of format between web feeds and web pages. Our solution for dates extraction extends authors \code{ScoreFunction} by comparing the \emph{extracted} string to multiple \emph{targets}, each being a different string representation of the original \emph{target} date obtained from the web feed. For instance, if the feed indicates that a post was published on \stringliteral{Thu, 01 Jan 1970 00:00:00}, our algorithm will search for a rule that returns one of \stringliteral{Thursday January 1, 1970}, \stringliteral{1970-01-01}, \stringliteral{43 years ago} and so on. We use a total of 8 target date formats that was empirically built during our experiments. While this list is certainly not exhaustive, our approach using string similarity on multiple target dates is tolerant to small variations. So far we do not support dates in multiple languages, but adding new target formats base and languages detection would be a simple extension of our date extraction algorithm.

% comments
Comments are usually available in separate web feeds, one per blog post. Similarly to blog feeds, comment feeds have a limited number of entries, and in case a blog post has more comments than the ones appearing in the feed, comments have to be extracted from the post web page. To do so we use the following \code{ScoreFunction}:
\begin{itemize}
  \item Rules returning less HTML nodes than the number of comments on the feed are filtered out with a zero score.
  \item Remaining rules are scored with the value of the \emph{maximum weighted matching} in the \emph{complete bipartite graph} $G = (U, V, E)$, where $U$ is the set of HTML nodes returned by the rule, $V$ is the set of target comment fields from the web feed (such as comment authors) and $E(u, v)$ as weight equal to \code{\ref{similarityAlgo}(}$u, v$\code{)}.
\end{itemize}
Our crawler executes this algorithm on each post with overflow on its comment feed, thus supporting blogs with multiple commenting engines. Comment contents are extracted first, which allows to narrow down the initial filtering by fixing a target number of comments.

\TODO{rush some time complexity analysis and conclude that we can do optimization analogs to algo 2 to obtain linear time on each extraction algo. }
