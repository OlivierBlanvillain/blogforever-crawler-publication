Web crawlers are complex software systems which often combine techniques from various disciplines in computer science. Our work on the BlogForever crawler is related to the fields of web data extraction, distributed computing and natural language processing. In the literature on web data extraction, the word *wrapper* is commonly used to designate procedures to extract structured data from unstructured documents. We did not use this word in the present paper in favour of the term *extraction rule*, which better reflects our implementation and is decoupled from software that concretely performs the extraction.

A common approach in web data extraction is to manually build wrappers for the targeted websites. This approach has been proposed for the crawler discussed in @faheem2012intelligent which automatically assigns web sites to predefined categories and gets the appropriate wrapper from a static knowledge base. The limiting factor in this type of approach is the substantial amount of manual work needed to write and maintain the wrappers, which is not compatible with the increasing size and diversity of the web. Several projects try to simplify this process and provide various degrees of automation. This is the case of the Stalker algorithm @stalker which generates wrappers based on user-labelled training examples. Some commercial solutions such as the Lixto project @lixto simplify the task of building wrappers by offering a complete integrated development environment where the training data set is obtained via a graphical user interface.

As an alternative to dedicated software for the creation and maintenance of wrappers, some query languages have been designed specifically for wrappers. These languages rely on their users to manually identify the structure of the data to be extraction. This structure can then be formalised as a small declarative program, which can then be turning into an concrete wrapper by an execution engine. The OXPath language @oxpath2013 is an interesting extension XPath designed to incorporate interaction in the extraction process. It supports simulation user actions such as filling forms or clicking button in order to obtain information that would not be accessible otherwise. Another extension of XPath, called Spatial XPath @sxpath2010, allows to write spacial rules in the extraction queries. The execution engine embeds a complete web browser which compute the visual representation of the page.

Fully automated solutions use different techniques to identify and extract information directly from the structure and content of the web page, without the need of any manual intervention. The Boilerpipe project @kohlschuetter2010 (mentioned in our evaluation) uses text density analysis to extract the main article of a web page. The approach presented in @treeedit is based on a tree structure analysis of pages with similar templates, such as news web sites or blogs. Automatic solutions have also been designed specifically for blogs. Similarly to our approach, Oita and Senellart @oita2010 describe a procedure to automatically build wrappers by matching web feed articles with HTML pages. This work was further extended by Gkotsis, Stepanyan, Cristea and Joy @gkotsis2013 with a focus on extracting content anterior to the one indexed in web feeds. @gkotsis2013 also reports to have successfully extracted blog post titles, publication dates and authors, but their approach is less generic than the one for the extraction of articles. Finally, neither @oita2010 nor @gkotsis2013 provide complexity analysis which we believe to be essential before using an algorithm in production.

One interesting research direction is the one of large scale distributed crawlers. Mercator @heydon99mercator, UbiCrawler @boldi2003 and the crawler discussed in @shkapenyuk2002 are examples of successful distributed crawlers. The associated articles provide useful information regarding the challenges encountered when working on a distributed architecture. One of the core issues when scaling out seems to be in sharing the list of URLs that have already been visited and those that need to be visited next. While @heydon99mercator and @shkapenyuk2002 rely on a central node to hold this information, @boldi2003 uses a fully distributed architecture where URLs are divided among nodes using consistent hashing. Both of these approaches require the crawlers to implement complex mechanisms to achieve fault tolerance. The BlogForever Crawler circumvents this problem by delegating all shared mutable state to the back-end system. In addition, since we process web pages on the fly and directly emit the extracted content to the back-end, there is no need for persistent storage on the crawler side. This removes one layer of complexity when compared to general crawlers which need to use a distributed file system (@shkapenyuk2002 uses NFS, @berger2011 uses HDFS) or implement an aggregation mechanism in order to further exploit the collected data. Our design is similar to the distributed active object pattern presented in @activeobject1996, which is further simplified by the fact that the state of the crawler instances is not kept between crawls.
