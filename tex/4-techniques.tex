\section{Techniques}

This section gives an overview of different techniques used in our crawler. We first show how we integrated a headless web browser into the harvesting process to support blogs that use JavaScript to display page content. The overall software architecture with then be discussed, introducing the Scrapy framework and the enrichments we implemented for our specific use case. The section will conclude with a discussion about how the crawler design leads nicely into a scalable and fault resilient distributed architecture.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{JavaScript rendering}
% Introduction
JavaScript is a widely used use language for client-side web scripting. While some applications simply use it for aesthetics aspects such as menus and animations, an increasing number of websites use JavaScript to download and display content. In such circumstances, traditional HTML based crawled would not see the web site as it would be presented to a human visitor and might therefore be obsolete for extracting data from the page.

% Motivation
In our specific use case of crawling the blog sphere, we encountered several blogs where data was missed because of the lack of JavaScript interpretation. The most frequent cases where blogs using the Disqus\cite{disqus2013} and LiveFyre\cite{livefyre2013} comment hosting services. These tools are very handy to setup for webmasters because the entire comments infrastructure is externalized and the setup essential comes down to including a JavaScript snippet in the target web page. Both of these services heavily rely on JavaScript to download and display the comments, even providing functionalities such as real-time update of comments there are written by users. Less commonly, some blogs are fully rendered using JavaScript. Concretely, when loading such website the web browser will not receive the page content inside an HTML page, but will instead have to execute a script containing the procedure to download and display the page content. On example can be found on the Blogger platforms which proposes as one of its default templates the \emph{Dynamic Views} \cite{antinharasymiv2011} that use this mechanism.

% The solution
To support blogs with JavaScript generated content we embed a full web browser into the crawler. After considering multiple option, we opted for PhantomJS\cite{phantomjs2013}, a headless web browser with great performances and scripting capabilities. In order to take full advantage of all the extraction functionalities, the JavaScript rendering is done as the very first step of blog post processing. This way, every other part of the project is independent of whether or not the page its currently processing was actually rendered by PhantomJS or comes directly from the web server. Extracting blog post content, comments and images therefore works equally well on both blogs with JavaScript generated content and traditional HTML blogs.

% Click click scripting
In addition to extracting page after JavaScript execution, we also use some of PhantomJS scripting capabilities. When the number of comments on one page exceeds a certain threshold, both Disqus and LiveFyre only load a subset of them and end the stream of comments with a \emph{Show More Comments} buttons. As part of the page load process we instruct PhantomJS to repeatedly click on this button until all comments are loaded. As you will see in the evaluation section, \TODO{we will see..}


%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Architecture}

\begin{figure}
  \capstart
  \centering
  \includegraphics[width=0.47\textwidth]{img/scrapy_architecture.png}
  \caption{Overview of the crawler architecture.\TODO{Source}}
  \label{architecture}
\end{figure}

Our project is build on top of Scrapy\cite{scra py2013}, an open source Python framework for web crawling. Scrapy provide an elegant and modulable high-level architecture which we followed in our implementation of the BlogForever Crawler. As illustrated in Figure~\ref{architecture}, Scrapy is divided in several components: \emph{Spiders}, \emph{Item Pipeline}, \emph{Downleader Middlewares} and \emph{Spider Middlewares}, where each of can be used to implement a different functionality.

Our use case has two types of spiders: \emph{NewCrawl} and \emph{UpdateCrawl}, which respectively implement the logic to crawl a new blog and to get updates from a previously crawled blog. After downloading the HTML page, blog posts are packed into Items and sent through the folloing pipeline of operation:
\begin{enumerate}[noitemsep]
  \item Render JavaScript
  \item Extract content
  \item Extract comments
  \item Download images
  \item Propagate resulting records to the Invenio backend
\end{enumerate}
This pipeline design, often called the \emph{pipes and filters pattern}\cite[Chapter Messaging Systems]{hohpe2003}, provides great modularity. Indeed, disabeling JavaScript rendering or plugging in an alternative backend can be done by editing a single configuration line.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Enriching Scrapy}
% Into, blog-post identification
In order to find and identify blog posts, our implementation enriches Scrapy with two components to narrow the extraction process to the subsets of blog pages holding blog posts: \emph{blog-post identification} and \emph{download priority heuristic}. Given an entry point to a website, the default Scrapy behavior allows to look over all pages of the same domain in a \emph{last in first out} manner. The \emph{blog-post identification} function is able to identify from a URL whether or not the corresponding page is a blog-post. Internally, this function uses a regular expression constructed for each blog from it's web feed entries. These constitute the only set of known blog post URLs, and can therefore be used identify the structure of blog post URLs. This simple approach requires that blogs use the same pattern for all posts (or false negative will occur) which has to be distinct for non post pages (or false positive will occur). In practice this assumption held for all blog platforms we encountered during the developments and seems to be a common practice among web developer.

% download priority heuristic
In order to efficiently deal with blogs with a large number non-blog-post pages, this blog-post identification mechanism is not sufficient. Indeed, after all pages identified as blog-post are processed, the crawler needs to download non-blog-post pages to search for additional blog-posts. To replace the naive random walk, depth first search or breadth first search web site traversals, we use a priority queue where new URL priorities are determined using a simple machine learning algorithm. Given an URL, the machine learning system does an estimation on the number of links to blog-post the corresponding page will contain. This estimated number of links are then used as a \emph{download priorities} to decide the best URL to visite next. Each time a page is effectively downloaded, the actual number of blog-post links it contains is computed and passed to the machine learning system as training data that will then be taken it into account during future predictions. This mechanism has shown to be indispensable for blogs hosting on the single domain a large number of non-blog web pages, such as a forum or a wiki.

Due to space constraints, more precise description and evaluation of these two components where not included in this paper.


%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Scalability}
When aiming to work with a large number of inputs, it is crucial that every layer of the system is build to be scalable. Citing the Reactive Manifesto\cite{thereactivemanifesto2013}:

\begin{quoting}
  If one of these layers does not participate—making blocking calls to the database, relying on shared mutable state, calling out to expensive synchronous operations—then the whole pipeline stalls and users will suffer through increased latency and reduced scalability.
\end{quoting}

At the time of writing we did not had the chance to work on a distributed deployment of the BlogForever project. However, the BlogForever crawler, and in particular the two core procedures \emph{NewCrawl} and \emph{UpdateCrawl}, where designed to be usable as part of a event-driven, scalable and fault resilient distributed system.

\Anotecontent{noteC}{Blogs that do not provide a global comment feed need a special handling. If comment feeds are attached to each blog post, periodically checking for comment updates might be done with a reasonable cost. However, in the worst case where no comment feeds are provided, the only solution is to periodically do a full crawl of the blog to capture new comments.}

Scalability and fault resilience are induced by the fact that both \emph{NewCrawl} and \emph{UpdateCrawl} are from a high level viewpoint stateless entities. \emph{NewCrawl} can be seen as a function, which takes as input the URL of a blog and returns structured records for blog-posts, comments and medias. Similarly, \emph{UpdateCrawl} takes as inputs the URL of blog\Anote{noteC} and a time, and returns records of objects that where emitted after the given time. The absence of state implies that in a distributed setup, crawler instances could be removed, added, and used interchangeably. Regarding the asynchronous architecture, Scrapy is build upon Twisted\cite{twisted2013}, an event-driven networking framework for Python. Scrapy also provides a built-in service to start spiders with an HTTP JSON API. With these tools in hand, the only component to implement in order to achieve a scalable and fault resilient distributed crawling system is an entity to keep track of available crawlers and does appropriate load balancing between them.
