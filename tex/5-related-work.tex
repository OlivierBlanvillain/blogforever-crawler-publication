\section{Related work}
\label{relatedwork}

Our crawler combines ideas from previous work on general web crawlers and \emph{wrapper} generation algorithms. The word \emph{wrapper} is commonly used to designate procedures to extract structured data from unstructured documents. We did not use this word in the present paper in favour of the term \emph{extraction rule}, which better reflects our implementation and is decoupled from the XPath engine that concretely performs the extraction.

Web crawling has been a well-studied topic over the past decade. One direction which we believe to be of crucial importance is the one of large scale distributed crawlers. Mercator \cite{heydon99mercator}, UbiCrawler \cite{boldi2003} and the crawler discussed in \cite{shkapenyuk2002} are examples of successful distributed crawler and the papers describing them provide useful information regarding the challenges encountered when working on a distributed architecture. One of the core issues when scaling out seems to be in sharing the list of URLs that have already been visited and the one that need to be visited next. While \cite{heydon99mercator} and \cite{shkapenyuk2002} rely on a central node to hold this information, \cite{boldi2003} uses a fully distributed architecture where URLs are divided among nodes using consistent hashing. Both of these approaches require the crawlers to implement complex mechanisms to achieve fault tolerance. The BlogForever Crawler circumvents this problem by delegating all shared mutable state to the back-end system. In addition, since we process web pages on the fly and directly emit the extracted content to the back-end, there is no need for persistent storage on the crawler side. This removes one layer of complexity when compared to general crawlers which need to use a distributed file system (\cite{shkapenyuk2002} uses NFS, \cite{berger2011} uses HDFS) or implement an aggregation mechanism in order to further exploit the collected data. Our design is similar to the distributed active object pattern presented in \cite{activeobject1996}, 
which is further simplified by the fact the state of crawler instances is not kept between crawls. \TODO{I think we should conclude this by insisting on the fact that we did not actually did a deployment on these kind of architecture, maybe justified by saying that this is out of the scope of this paper}

A common approach in web content extraction is to manually build wrappers for the targeted websites. This approach has been proposed in the crawler discussed in \cite{faheem2012intelligent} which automatically categorises web sites according to the technology they use \TODO{something better than "technology they use"...}, and uses this information to obtain an appropriate wrapper from a knowledge base. The limiting factor in this type of approach is the substantial amount of manual work needed to write and maintain the wrappers, which is not compatible with the increasing size and diversity of the web. Several works try to simplify this process and provide various degree of automation. This is the case of the Stalker algorithm \cite{stalker} which generate wrappers based on user-labelled training examples. Some commercial solution such as the Lixto project \cite{lixto} simplify the task of building wrappers by offering a full integrated development environment where the training data set is obtained via a graphical user interface.

Fully automatic solutions have used other techniques to identify and extract the content of interest directly from the structure and content of web page. The Boilerpipe project \cite{kohlschuetter2010} mentioned on our evaluation uses text density analysis to extract the main article of a web page. The approach presented in \cite{treeedit} is based on tree structure analysis between pages with similar templates such a news sites or blogs. Automatic solutions have also been designed specifically for blogs. Similarly to your approach, Oita and Senellart \cite{oita2010} describe a procedure to automatically build wrappers by matching article of web feeds with HTML pages. This work was further extended by Gkotsis, Stepanyan, Cristea and Joy \cite{gkotsis2013} with a focus on extracting content anterior to the one indexed web feeds. \cite{gkotsis2013} also reports to have successfully extracted blog post titles, publication dates and authors, but their approach is less generic than for the extraction of articles. Finally, neither \cite{oita2010} nor \cite{gkotsis2013} provide complexity analysis which we believe to be essential before putting an algorithm in production.

