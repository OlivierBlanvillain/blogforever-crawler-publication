\section{Related work}
\label{relatedwork}

Web crawling has been a well-studied topic over the last decade. One direction of study which we believe to be of crucial importance is the one of large scale distributed crawlers. Our design is influenced by the work Heydon and Najork work on Mercator \cite{heydon99mercator}, Boldi et al. work's on UbiCrawler \cite{boldi2003} and Shkapenyuk and Suel work's on their spider discussed in \cite{shkapenyuk2002}. One of the core issues when scaling out a crawler seems to be in sharing the list URLs that have already been visited and need to be visited next. While \cite{heydon99mercator} and \cite{shkapenyuk2002} rely on a central node to hold this information, \cite{boldi2003} uses a fully distributed architecture where URLs are divided among nodes using consistent hashing. Both of these approaches require complex mechanisms to achieve fault tolerance.

The BlogForever Crawler -circumvent- delegates this problem by delegating all state the back-end system. This shared nothing architecture between crawler instance is made possible by the fact that our unit of computations, crawling a entire blog, is much large than fetching a single URL and does not generate further crawling tasks. In addition, because we process web pages on the fly and directly emit the extracted content to the back-end, there is no need for persistent storage on the crawler side. This removes one layer of complexity when compared to general crawler which need use a distributed file system or implement an aggregation mechanism in order to further use the collected data.

\cite{shkapenyuk2002} used NFS, \cite{berger2011} uses HDFS

- Wrapper generation\\
these works do not clearly address how to extract other info link author and publication date, and do not include complexity analysis which is something that cannot be ignored when aiming for a universal large scale solution.

\comment{One very common approach for web crawling is to follow different crawling techniques for different types of web applications and different platforms. Application-aware crawling is based on a knowledge base of web applications and their corresponding special web crawling instructions. The drawback of this type of approach is that it does not scale and it cannot cope with the increasing number and diversity of web applications. \cite{faheem2012intelligent}}



\Anotecontent{wrapper}{In the literature, procedures to extract structured data from unstructured document are commonly referred as \emph{wrappers}. In this paper we decided to rather employ the less generic term of \emph{extraction rules} which is closer to our implementation.}

The BlogForever Crawler combines ideas from previous work on general web crawlers and extraction rule\Anote{wrapper} generation algorithms.



Several experiments have been conducted in 

There have been several successful research working design distributed crawlers 





% Web archiving is a challenging and very interesting problem and has been the subject of many studies as well as the goal of several projects.
% \comment{Mention Internet Archive here and maybe another generic web archiving project}

% A fraction of web archiving studies and projects focus on the specific problem of blog harvesting and archiving.
% They focus on the unique nature of blogs as dynamic web resources with a usually common set of attributes.
% Each entry in a blog, commonly called a post, has a title, an author, a publication date, some content and possibly some descriptive tags.
% (In this section we review some studies and projects that introduce and describe such techniques.)

% Blogs are strongly associated with web feeds, using technologies such as RSS and Atom (expand and reference - needed?).
% Web feeds are structured documents (often XML-based) that advertize and provide access to a website's content.
% Given their format, web feeds constitute an excellent source for blog archiving.
% They do, however, present some problems: (a) web feeds contain only the latest posts of a blog (usually 20) and (b) web feeds may only provide a summary or a fraction of the entire content of each blog post.
% More advanced methods and techniques have to be employed in order to achieve quality blog archiving.

% Here mention projects that use web feeds for discovery and compare them to ours.
% \begin{itemize}
%   \item ( Zero-cost Labelling with Web Feeds for Weblog Data Extraction - http://www2013.org/companion/p73.pdf).
%   \item ( Self-supervised Automated Wrapper Generation for Weblog Data Extraction - https://github.com/OlivierBlanvillain/blogforever-crawler-publication/raw/master/papers/bncod\_published.pdf).
%   \item ( Archiving Data Objects using Web Feeds - http://hal.archives-ouvertes.fr/docs/00/53/79/62/PDF/iwawienna.pdf).
% \end{itemize}
% Some of these should also be in the comparison for extraction methods.

% Here mention projects that use other techniques for discovery and compare them to ours.
% \begin{itemize}
%   \item ???
% \end{itemize}

% Here mention projects that use various techniques for extraction and compare them to ours.
% \begin{itemize}
%   \item ( Web Object Identification for Web Automation and Meta-Search - http://www.dbai.tuwien.ac.at/proj/tamcrow/download/Kordomatis2013WIMS.pdf).
%   \item ( OXPath: A Language for Scalable, Memory-efficient Data Extraction from Web Applications - http://www.vldb.org/pvldb/vol4/p1016-furche.pdf).
%   \item ( Intelligent and Adaptive Crawling of Web Applications for Web Archiving - http://pierre.senellart.com/publications/faheem2013intelligent.pdf).
%   \item ( Web Data Extraction, Applications and Techniques: A Survey - http://www.emilio.ferrara.name/wp-content/uploads/2011/07/survey-csur.pdf).
% \end{itemize}

% Here mention that there is no comparison for the JavaScript rendering as there are no similar projects.
% \begin{itemize}
%   \item ???
% \end{itemize}
