\section{Related work}
\label{relatedwork}

Our crawler combines ideas from previous work on general web crawlers and \emph{wrapper} generation algorithms. The word \emph{wrapper} is commonly used to designate procedures to extract structured data from unstructured documents. We did not employ\comment{emply VS. use?} this word in the present paper in favour of the term \emph{extraction rule}, which better reflects our implementation and is decoupled from the XPath engine that concretely performs the extraction.

Web crawling has been a well-studied topic over the past decade. One direction which we believe to be of crucial importance is the one of large scale distributed crawlers. Mercator \cite{heydon99mercator}, UbiCrawler \cite{boldi2003} and the crawler discussed in \cite{shkapenyuk2002} are examples of successful distributed crawlers and the papers describing them provide useful information regarding the challenges encountered when working on a distributed architecture. One of the core issues when scaling out seems to be in sharing the list of URLs that have already been visited and the ones that need to be visited next. While \cite{heydon99mercator} and \cite{shkapenyuk2002} rely on a central node to hold this information, \cite{boldi2003} uses a fully distributed architecture where URLs are divided among nodes using consistent hashing. Both of these approaches require the crawlers to implement complex mechanisms to achieve fault tolerance.
%
The BlogForever Crawler circumvents this problem by delegating the state of all the blogs to the back-end system. This shared nothing architecture between crawler instance is made possible by the fact that our units of computation, crawling entire blogs, is much large than fetching single URLs and does not generate further crawling tasks.\comment{NK: This is last sentence is a bit hard to understand. Maybe rephrase to something like: "This architectural choice where nothing is shared among the crawler instances is made possible by the fact that our (basic/primary) unit of computation is the entrire blog, instead of single URLs to blog pages, avoiding the generation of further crawling tasks."?}
%
In addition, since we process web pages on the fly and directly emit the extracted content to the back-end, there is no need for persistent storage on the crawler side. This removes one layer of complexity when compared to general crawlers which need to use a distributed file system (\cite{shkapenyuk2002} uses NFS, \cite{berger2011} uses HDFS) or implement an aggregation mechanism in order to further exploit the collected data. Our design is inspired by the ideas of distributed active object presented in \cite{activeobject1996}.



A common approach in web content extraction is to manually build wrappers for the targeted websites. This approach has been proposed in the crawler discussed in \cite{faheem2012intelligent} which automatically categorises web sites according to the technology they use and uses this information to obtain an appropriate wrapper from a knowledge base. The limiting factor in this type of approach is the substantial amount of manual work needed to write and maintain the wrappers, which is not compatible with the increasing size and diversity of the web.


these works do not clearly address how to extract other info link author and publication date, and do not include complexity analysis which is something that cannot be ignored when aiming for a universal large scale solution.

