In this paper, we presented the internals of the BlogForever Crawler. Its central article extraction procedure based on extraction rules generation was introduced along with theoretical and empirical evidence validating the approach. A simple adaptation of this procedure that allows to extract different types of content, including authors, dates and comments was then presented. In order to support rapidly evolving web technologies such as JavaScript-generated content, the crawler uses a web browser to render pages before processing them. We also discussed the overall software architecture, highlighting the design choices made to achieve both modularity and scalability. Finally, we evaluated our content extraction algorithm against three state-of-the-art web article extraction algorithms.

Future work could investigate *hybrid* content extraction algorithms to try and achieve near 100\% success rates. Indeed, we have observed[^1] that the primary causes of failure of our approach were the insufficient quality of web feeds or the high amount of structural variations of blog pages. This suggests that combining our approach with other techniques such as word density or spacial reasoning could lead to better performance given that these techniques are insensible to the above issues.

Another possible research direction would be the deployment of the BlogForever Crawler on a large scale distributed system. This is particularly relevant in the domain of web crawling given that intensive network operations can be a serious bottleneck. Crawlers greatly benefit from the use of multiple Internet access points which makes them natural candidates for distributed computing. We intend to explore these opportunities in our future work.

[^1]: An in-depth analysis of causes of failure was not included in this paper given the high amount of manual work required to identify causes of failure on problematic pages.
