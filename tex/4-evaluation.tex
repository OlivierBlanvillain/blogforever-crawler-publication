\section{Evaluation}
\label{evaluation}

Our evaluation is articulated in two parts. First, we compare the article extraction procedure presented in \autoref{algorithms} with three open-source projects capable of extracting article and title from web pages. The comparison will show that our blog-targeted solution has better performance both in terms of success rate and running time. Second, a discussion is held regarding the different solutions available to archive data beyond what is available in the HTML source code. Extraction of authors, dates and comments is not part of this evaluation because of the lack of publicly available competing projects and reference data sets.

In our experiments we used \emph{Debian GNU/Linux 7.2}, \emph{Python 2.7} and an \emph{Intel Core i7-3770 3.4 GHz} processor. Timing measurements were made on a single dedicated core with garbage collection disabled. The Git repository for this paper\footnote{\url{https://github.com/OlivierBlanvillain/bfc-paper}} contains the necessary scripts and instructions to reproduce all the evaluation experiments presented in this section. The crawler source code is available under the MIT license from the project's websites\footnote{\url{https://github.com/BlogForever/crawler}}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Extraction success rates}
To evaluate article and title extraction from blog posts we compare our approach to three open source projects: Readability \footnote{\url{https://github.com/gfxmonk/python-readability}}, Boilerpipe \cite{kohlschuetter2010} and Goose\footnote{\url{https://github.com/GravityLabs/goose}}, implemented in JavaScript, Java and Scala respectively. These projects are more generic than our blog-specific approach in the sense that they are able to identify and extract data directly from HTML source code, and do not make use of web feeds or structural similarities between pages of the same blog (observations \ref{havefeedAssum} and \ref{similarhtmlAssum}). \autoref{precisionTable} shows the extraction success rates for article and title on a test sample of 2300 blog posts from 230 blogs obtained from the Spinn3r dataset \cite{burton2011}.

\precisionTable

On our test dataset \autoref{extractionAlgo} outperformed the competition by 4.9\% on article extraction and 10.1\% on title extraction. It is important to stress that Readability, Boilerpipe and Goose rely on generic techniques such as word density, paragraph clustering and heuristics on HTML tagging conventions, which are designed to work for any type of web page. On the contrary, our algorithm is only suitable for pages with associated web feeds, as these provide the reference data used to build extraction rules. Therefore, results showed in \autoref{precisionTable} should not be interpreted as a general quality evaluation of the different projects, but simply as an evidence that our approach is more suitable when working with blogs.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Article extraction running times}

In addition to the quality of the extracted data we also evaluated the running time of the extraction procedure. The main point of interest is the ability of the extraction procedure to scale as the number of posts in the processed blog increases. This corresponds to the evaluation of a \emph{NewCrawl} task, which is in charge of harvesting all published content on a blog.

\autoref{runningtime} shows the cumulated time spent for each article extraction procedure (this excludes common tasks such as downloading pages and storing results) as a function of the number of blog posts processed. We used the Quantum Diaries\footnote{\url{http://www.quantumdiaries.org}} blog for this experiment.

Data presented in this graph was obtained by taking the arithmetic mean over 10 measurements. These results are believed to be significant given that standard deviations are of the order of 2 milliseconds.

\input{runningtimeFigure.tex}

As illustrated in \autoref{runningtime}, our approach spends the majority of it's total running time between the initialisation the processing of the first blog post. This initial increase of about 0.4 seconds corresponds to cost of executing \autoref{linearAlgo} to compute extraction rule for articles. As already mentioned, this consists in computing the \emph{best extraction rule} of each pages references by the web feed and picking the one functioning best on these pages. Once we have this extraction rule, processing subsequent blog posts only requires parsing and applying the rule, which takes about 3 milliseconds and are barely visible on the scale of \autoref{runningtime}. The other evaluated solutions do not function this way: each blog post is processed as new and independent input, leading to approximately linear running times.

The vertical dashed line at 15 processed blog posts represents a suitable point of comparison of processing time per blog post. Indeed, as the web feed of our test blog contains 15 blog posts, the extraction rule computation performed by our approach include the cost of entirely processing these 15 entries. That being said, comparing raw performance of algorithms implemented in different programming languages is not very informative given the high variations of running times observed across programming languages \cite{hundt2011}.
