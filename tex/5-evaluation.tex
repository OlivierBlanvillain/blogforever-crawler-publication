\section{Evaluation}

Our evaluation will be articulated in two parts. Firstly, we will compare the content extraction procedure presented in section \ref{algorithms} with three open source projects capable of generically extracting content from web articles. The comparison will show that our blog targeted solution has better performances both in term of running time and success rates. Secondly, a discussion will be held regarding the different solutions available to archive data beyond what is available in HTML source code. Extraction of authors, dates and comments is not part of evaluation because of the lack of publicly available competing projects and reference data sets.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Content extraction Precision}
To evaluate of post content and title extraction we compared our approach to three open source projects: Readability\cite{}, Boilerpipe\cite{kohlschuetter2010} and Goose\cite{}. These projects are more generic than our blog specific approach in the sense that they are able to identify and extract data directly from HTML source code, and do not make use of web feeds or structural similarities between pages of a same blog (assumptions \ref{havefeedAssum} and \ref{similarhtmlAssum}).

\comment{Is it useful to present algorithms of the three other projects?
\\ Readability relies mainly on common naming practices among web-developers
\\ Boilerpipe: http://tomazkovacic.com/blog/14/extracting-article-text-from-html-documents/
\\ Goose clusters of paragraphs, word dencity and link dencity}

Table \ref{precisionTable} shows extraction success rates of content and title on a test sample of 2300 blog posts obtained from the Spinn3r dataset \cite{burton2011}. An extraction was considered successful when the returned string is a least 0.5 similar to the reference string, with respect to the Sørensen–Dice coefficient similarity.

\precisionTable

% short discussion, stress the fact that we use different tools



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Content running time}

Implemented in Javascript, Java and Scala respectively, all three offer the same functionality: given an HTML web page, identify and extract content, tit.
% rsetup (single core, gc disabled, measure time spent in respective extraction functions)
% result (plot)
\input{runningtimesfigure.tex}


% discussion: we are "cheating" on scalability
% discussion: raw perf (15 feed entry)
% discussion: (c++ vs java vs scala vs go \cite{hundt2011})


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{JavaScript rendering}

% intro, plan (why we pick these projects, what we want to show with this evaluation)

% \item "Wget crawl" http://blogforever.eu/blog/2011/05/21/creating-a-snapshot-of-a-blog-post-using-wget/
% \item wkhtmltopdf http://code.google.com/p/wkhtmltopdf/ http://blogforever.eu/blog/2011/05/17/rendering-and-storing-web-pages-using-wkhtmltopdf/
% \item OXPath
% \item Selenium + full browser

% table to recap

% discussion (ez of use vs power, fine because it's automated)

