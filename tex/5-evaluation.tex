\section{Evaluation}

Our evaluation is articulated in two parts. First, we compare the article extraction procedure presented in \autoref{algorithms} with three open-source projects capable of extracting article and title from web pages. The comparison will show that our blog-targeted solution has better performance both in terms of success rate and running time. Second, a discussion is held regarding the different solutions available to archive data beyond what is available in the HTML source code. Extraction of authors, dates and comments is not part of this evaluation because of the lack of publicly available competing projects and reference data sets.

In our experiments we used \emph{Debian GNU/Linux 7.2}, \emph{Python 2.7} and an \emph{Intel Core i7-3770 3.4 GHz} processor. Timing measurements were made on a single dedicated core with garbage collection disabled. The Git repository for this paper \cite{repositoryofthispaper} contains the necessary scripts and instructions to reproduce all the evaluation experiments presented in this section.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Extraction success rates}
To evaluate article and title extraction from blog posts we compared our approach to three open source projects: Readability \cite{python-readability2011}, Boilerpipe \cite{kohlschuetter2010} and Goose \cite{goose2012}, implemented in JavaScript, Java and Scala respectively. These projects are more generic than our blog-specific approach in the sense that they are able to identify and extract data directly from HTML source code, and do not make use of web feeds or structural similarities between pages of the same blog (observations \ref{havefeedAssum} and \ref{similarhtmlAssum}). \autoref{precisionTable} shows the extraction success rates for article and title on a test sample of 2300 blog posts obtained from the Spinn3r dataset \cite{burton2011}.

% An extraction was considered successful when the returned string is a least 0.5 similar to the reference string, with respect to the Sørensen–Dice coefficient similarity.

\precisionTable

On our test dataset \autoref{extractionAlgo} outperformed the competition by 4.9\% on article extraction and 10.1\% on title extraction. It is important to stress that Readability, Boilerpipe and Goose rely on generic techniques such as word density, paragraph clustering and heuristics on HTML tagging conventions, which are designed to work for any type of web page. Contrariwise\comment{:="in the opposite way or order." Maybe switch to something like "On the contrary"?}, our algorithm is only suitable for pages with associated web feeds, as these provide the reference data which we search for in HTML trees of pages\comment{this last sentence is a bit hard to understand.}. Therefore, results showen in \autoref{precisionTable} should not be interpreted as a general quality evaluation of the different projects, but simply as an demonstration\comment{or evidence or proof ?} that our approach is more suitable when working with blogs.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Article extraction running times}

% running time eval, scalability with size of blogs
In addition to the quality of the extracted data we also evaluated the running time of the extraction procedure. The main point of interest is the ability of the extraction procedure to scale as the number of posts in the processed blog increases. This corresponds to the evaluation of a \emph{NewCrawl} task, which is in charge of harvesting all published content on a blog.

% setup
Experimental data was obtained by taking the arithmetic mean over 10 measurements, which we believe to be significant given that each result has a standard deviations on the order of 2 milliseconds. \comment{Do I have to justify the arithmetic mean? If yes, this could do: \surl{http://dl.acm.org/citation.cfm?id=5673}}\comment{need to rephrase - let's discuss.}

\autoref{runningtime} shows the cumulated time spent for each article extraction procedure (this excludes common tasks such as downloading pages and storing results) as a function of the number of blog posts processed. The reference blog we used was \surl{http://quantumdiaries.org/}.

\input{runningtimeFigure.tex}

Past the initial computations, the cost to extract an article with our approach is almost zero. This is a consequence of having a blog-aware, rule-based algorithm. As already mentioned in \autoref{algorithms}, the central idea is to first build extraction rules using the information provided by the web feed, and then use these rules on all posts of the blog. In this example, the cost, in time, for parsing a new blog post and applying the extraction rules is in the order of 3 milliseconds, which is barely not visible on the scale of \autoref{runningtime}. The three other evaluated solutions do not function this way: each blog post is processed as new and independent input, leading to close to linear running times.

In \autoref{runningtime}, the vertical dashed line at 15 processed blog posts represents a suitable point a comparison of processing time per blog post. Indeed, as the test blog's web feed contains 15 blog posts, the time spent on processing the first blog post also includes the processing time of the web feed and its 15 entries. \comment{Is this clear? --> almost, let's discuss}That being said, comparing raw performance of different algorithms implemented in different programming languages is not very informative given the high variations of running times observed across languages for identical algorithms \cite{hundt2011}.

% python x23 just-in-time compilation speed up http://dl.acm.org/citation.cfm?id=2069181


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{JavaScript rendering}

% intro, plan (why we pick these projects, what we want to show with this evaluation)

% \item "Wget crawl" http://blogforever.eu/blog/2011/05/21/creating-a-snapshot-of-a-blog-post-using-wget/
% \item wkhtmltopdf http://code.google.com/p/wkhtmltopdf/ http://blogforever.eu/blog/2011/05/17/rendering-and-storing-web-pages-using-wkhtmltopdf/
% \item OXPath
% \item Selenium + full browser

% table to recap

% discussion (ez of use vs power, fine because it's automated)

